{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics-informed neural network for 3D inverse modeling of natural-state geothermal systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Published on February 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The program was implemented under the following versions:\n",
    "# Tensorflow version 2.9.1\n",
    "# Tensorflow probability version 0.17.0\n",
    "# Numpy version 1.21.1\n",
    "# pandas version 1.4.3\n",
    "# matplotlib version 3.5.0\n",
    "# scipy version 1.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./\")\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "import sys\n",
    "from time import time\n",
    "from enum import Enum\n",
    "from scipy import interpolate\n",
    "from scipy.interpolate import Rbf\n",
    "import scipy.optimize\n",
    "import os\n",
    "import shutil\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "from optimizers.lbfgs import LBfgsOptimizer\n",
    "from utils import rearrange_coordinate_container as rcc\n",
    "# \n",
    "script_dir = os.path.abspath('') + \"/utils/\"\n",
    "sys.path.append(script_dir)\n",
    "ipt_dir = os.path.abspath('') + \"/utils/\"\n",
    "sys.path.append(script_dir)\n",
    "import set_wells_train_val as swtv\n",
    "import get_boundary_indexlist as gb\n",
    "import get_tkp_on_new_coords as gtpk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "gpu_id = 0\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_visible_devices(gpus[gpu_id], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeumannBcApproximater:\n",
    "    \"\"\"\n",
    "    A class of approximate functions for Neumann boundary conditions on 3-dimensional rectangular domains.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : Tensor\n",
    "        x (x1) coordinate of the surface on which the Neumann boundary condition is imposed.\n",
    "        shape = (number_of_points, 1)\n",
    "    Y : Tensor\n",
    "        y (x2) coordinate of the surface on which the Neumann boundary condition is imposed.\n",
    "        shape = (number_of_points, 1)\n",
    "    var : Tensor\n",
    "        Physical quantities of the surface imposing Neumann boundary conditions\n",
    "        shape = (number_of_points, 1)\n",
    "    order_interpolate : int\n",
    "        The order of interpolation of the boundary conditions\n",
    "    regularization_interpolate : float\n",
    "        Regularization weight for interpolation of boundary conditions\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    XY : Tensor\n",
    "        Reshaped tensor for XY coordinates\n",
    "        shape = (1, number_of_points, 2)\n",
    "    var  : Tensor\n",
    "        Reshaped tensor for a physical quantity\n",
    "        shape = (1, number_of_points, 1)\n",
    "    \"\"\"\n",
    "    def __init__(self, X, Y, var, order_interpolate, regularization_interpolate):\n",
    "        size = X.numpy().shape[0]\n",
    "\n",
    "        self.XY = tf.reshape(tf.concat([X, Y], axis = 1), [1, size, 2])\n",
    "        self.var = tf.reshape(var, [1, size, 1])\n",
    "        self.order = order_interpolate\n",
    "        self.reg = regularization_interpolate\n",
    "\n",
    "    def function(self, x, y):\n",
    "        \"\"\"\n",
    "        Approximated function for a quantity on the Neumann boundaries\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        x : tf.Tensor\n",
    "        Tensor representing the x-coordinate of the position at which the interpolation amount is calculated\n",
    "        shape = (number_of_points, 1)\n",
    "        y : tf.Tensor\n",
    "        Tensor representing the y-coordinate of the position at which the interpolation amount is calculated\n",
    "        shape = (number_of_points, 1)\n",
    "        \n",
    "        Output\n",
    "        ------\n",
    "        tf.Tensor\n",
    "        Returns the physical quantity calculated by interpolation.\n",
    "        shape = (1, number_of_points, 1)\n",
    "        \"\"\"\n",
    "        size = x.shape[0]\n",
    "        x_reshaped = tf.reshape(x, [1, size, 1])\n",
    "        y_reshaped = tf.reshape(y, [1, size, 1])\n",
    "        xy = tf.reshape(tf.concat([x_reshaped, y_reshaped], axis = 1), [1, size, 2])\n",
    "        var = tfa.image.interpolate_spline(self.XY, self.var, xy, self.order, regularization_weight = self.reg)\n",
    "        return tf.reshape(var, x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GridSpec:\n",
    "    \"\"\"\n",
    "    Data class that holds information about grid specifications\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    N_x1: int\n",
    "      Number_of_points in the first axis\n",
    "    N_x2: int\n",
    "      Number_of_points in the second axis\n",
    "    N_x3: int\n",
    "      Number_of_points in the third axis\n",
    "    up_indices: np.ndarray\n",
    "      An array of indices that refers to points on the upper surface boundary.\n",
    "      shape = (num_upper_bound_points, ndim=3)\n",
    "    low_indices: np.ndarray\n",
    "      An array of indices that refers to points on the bottom boundary.\n",
    "      shape = (num_lower_bound_points, ndim=3)\n",
    "    side_indices: np.ndarray\n",
    "      An array of indices that refers to points on the side boundaries.\n",
    "      shape = (num_side_bound_points, ndim=3)\n",
    "    \"\"\"\n",
    "    N_x1: int\n",
    "    N_x2: int\n",
    "    N_x3: int\n",
    "\n",
    "    up_indices: np.ndarray\n",
    "    low_indices: np.ndarray\n",
    "    side_indices: np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "class PINN_NeuralNet3(tf.keras.Model):\n",
    "    \"\"\" Set basic architecture of the PINN model.\"\"\"\n",
    "\n",
    "    def __init__(self, lb, ub,\n",
    "            output_dim=1,\n",
    "            num_hidden_layers=4,\n",
    "            num_neurons_per_layer=50,\n",
    "            activation='tanh',\n",
    "            kernel_initializer='glorot_normal',\n",
    "            **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "\n",
    "        # Define NN architecture\n",
    "        self.scale1 = tf.keras.layers.Lambda(\n",
    "            lambda x: 2.0*(x - lb)/(ub - lb) - 1.0)\n",
    "        self.hidden1 = [tf.keras.layers.Dense(num_neurons_per_layer,\n",
    "                             activation=tf.keras.activations.get(activation),\n",
    "                             kernel_initializer=kernel_initializer)\n",
    "                           for _ in range(self.num_hidden_layers)]\n",
    "        self.out1 = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "        self.scale2 = tf.keras.layers.Lambda(\n",
    "            lambda x: 2.0*(x - lb)/(ub - lb) - 1.0)\n",
    "        self.hidden2 = [tf.keras.layers.Dense(num_neurons_per_layer,\n",
    "                             activation=tf.keras.activations.get(activation),\n",
    "                             kernel_initializer=kernel_initializer)\n",
    "                           for _ in range(self.num_hidden_layers)]\n",
    "        self.out2 = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "        self.scale3 = tf.keras.layers.Lambda(\n",
    "            lambda x: 2.0*(x - lb)/(ub - lb) - 1.0)\n",
    "        self.hidden3 = [tf.keras.layers.Dense(num_neurons_per_layer,\n",
    "                             activation=tf.keras.activations.get(activation),\n",
    "                             kernel_initializer=kernel_initializer)\n",
    "                           for _ in range(self.num_hidden_layers)]\n",
    "        self.out3 = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, X):\n",
    "        \"\"\"Forward-pass through neural network.\"\"\"\n",
    "        Z1 = self.scale1(X)\n",
    "        Z2 = self.scale2(X)\n",
    "        Z3 = self.scale3(X)\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            Z1 = self.hidden1[i](Z1)\n",
    "            Z2 = self.hidden2[i](Z2)\n",
    "            Z3 = self.hidden3[i](Z3)\n",
    "        return self.out1(Z1),self.out2(Z2),self.out3(Z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableType(Enum):\n",
    "    \"\"\"\n",
    "    Enumeration type that represents the type of physical quantity (T, p, k, etc.)\n",
    "    \"\"\"\n",
    "    T = 1\n",
    "    p = 2\n",
    "    k = 3\n",
    "    others = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Enum):\n",
    "    \"\"\"\n",
    "     Enumeration type that represents the type of datasets (training , validation, all) \n",
    "    \"\"\"\n",
    "    TRAIN = 0\n",
    "    VAL = 1\n",
    "    ALL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINNSolver_3D():\n",
    "\n",
    "    \"\"\"_summary_\n",
    "    Class that performs PINN optimization\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        model: class\n",
    "                 Class of the physics-informed neural network\n",
    "        model_input_grid_spec: Grid specification\n",
    "        if_low_dirichlet: bool\n",
    "                Bool type indicating whether or not to impose the Dirichlet condition on the bottom surface.\n",
    "        PI_indices: Tensor\n",
    "                The index of the collocation points used to calculate the losses for mass and energy balances (PI1loss and PI2loss)\n",
    "        X_uTpbD: Tensor\n",
    "                Coordinates of the upper surface boundary used for Dirichlet boundary condition\n",
    "                shape = (Num_surface_bound, 3)\n",
    "        X_lTpbD: Tensor\n",
    "                Coordinates of the bottom boundary used for Dirichlet boundary condition\n",
    "                shape = (Num_bottom_bound, 3)\n",
    "        nvec_p: Tensor\n",
    "                Normal vector for side boundaries\n",
    "                shape = (Num_side_bounds, 3)\n",
    "        X_TbN: Tensor\n",
    "                Corrdinates for the Neumann boundary condition at the bottom\n",
    "                shape = (Num_bottom_bound, 3)\n",
    "        nvec_T: Tensor\n",
    "                Normal vector at the bottom\n",
    "                shape = (Num_bottom_bound, 3)\n",
    "        Y_uTpbD_T: Tensor\n",
    "                Temperatures specified at the upper surface used as the Dirichlet boundary condition\n",
    "        Y_uTpbD_p: Tensor\n",
    "                Pressures specified at the upper surface used as the Dirichlet boundary condition\n",
    "                shape = (Num_surface_bound, 1)\n",
    "        Y_lTpbD_T: Tensor\n",
    "                Temperatures specified at the bottom used as the Dirichlet boundary condition\n",
    "                shape = (Num_bottom_bound, 1)\n",
    "        Y_lTpbD_p: Tensor\n",
    "                Pressures specified at the bottom used as the Dirichlet boundary condition\n",
    "                shape = (Num_bottom_bound, 1)\n",
    "        cof_PI1: float\n",
    "                Coefficient for the loss of mass balance\n",
    "        cof_PI2: float\n",
    "                Coefficient for the loss of energy balance\n",
    "        cof_pbN: float\n",
    "                Coefficient for the Neumann boundary at the sides\n",
    "        cof_TbN: float\n",
    "                Coefficient for the Neumann boundary at the bottom\n",
    "        T_true: nd.array\n",
    "                Reference temperature\n",
    "                shape = (Num_all_grid, 1)\n",
    "        p_true: nd.array\n",
    "                Reference pressure\n",
    "                shape = (Num_all_grid, 1)\n",
    "        k_true: nd.array\n",
    "                Reference permeability\n",
    "                shape = (Num_all_grid, 1)\n",
    "        lmode: int\n",
    "                The calculation mode specified\n",
    "        dtype: Data type\n",
    "    \"\"\"\n",
    "    def __init__(self, model, model_input_grid_spec: GridSpec,\n",
    "                if_low_dirichlet,\n",
    "                PI_indices,\n",
    "                X_uTpbD, X_lTpbD,\n",
    "                X_pbN, nvec_p, X_TbN, nvec_T,\n",
    "                X_all,\n",
    "                Y_uTpbD_T, Y_uTpbD_p, Y_lTpbD_T, Y_lTpbD_p,\n",
    "                cof_PI1AD, cof_PI2AD, cof_pbN, cof_TbN,\n",
    "                T_true, p_true, k_true,\n",
    "                T_for_normalize, p_for_normalize, k_for_normalize,\n",
    "                lmode,dtype,\n",
    "                order_interpolate = 1,\n",
    "                regularization_interpolate = 1e-4):\n",
    "        \n",
    "        self.model = model\n",
    "\n",
    "        # Information about the input grid\n",
    "        self._model_input_grid_spec: GridSpec = model_input_grid_spec\n",
    "\n",
    "        # Generate the grid coordinates that is input when making a model prediction\n",
    "        self._model_input_grid = tf.transpose(\n",
    "            tf.reshape(X_all,\n",
    "                       shape=(model_input_grid_spec.N_x3,\n",
    "                              model_input_grid_spec.N_x2,\n",
    "                              model_input_grid_spec.N_x1,\n",
    "                              3)\n",
    "            ),\n",
    "            perm=[2, 1, 0, 3]\n",
    "        )[tf.newaxis, ...]\n",
    "\n",
    "        # Generate indices for extracting collocation points from the input grid\n",
    "        self._grid_collocation_point_indices = (\n",
    "            tf.gather(\n",
    "                tf.transpose(\n",
    "                    tf.unravel_index(PI_indices,\n",
    "                                    dims=(model_input_grid_spec.N_x3,\n",
    "                                          model_input_grid_spec.N_x2,\n",
    "                                          model_input_grid_spec.N_x1))),\n",
    "                indices=[2, 1, 0],\n",
    "                axis=1)\n",
    "        )\n",
    "\n",
    "        # Bool type for the bottom boundary condition\n",
    "        self.if_low_dirichlet = if_low_dirichlet\n",
    "\n",
    "        # Store boundary points\n",
    "        self.xuTpbD = X_uTpbD\n",
    "        self.xlTpbD = X_lTpbD\n",
    "\n",
    "        self.xpbN1 = X_pbN[:,0:1]\n",
    "        self.xpbN2 = X_pbN[:,1:2]\n",
    "        self.xpbN3 = X_pbN[:,2:3]\n",
    "\n",
    "        self.nvecp = nvec_p\n",
    "        self.xTbN1 = X_TbN[:,0:1]\n",
    "        self.xTbN2 = X_TbN[:,1:2]\n",
    "        self.xTbN3 = X_TbN[:,2:3]\n",
    "        self.nvecT = nvec_T\n",
    "        self.xall = X_all\n",
    "\n",
    "        self.Y_uT = Y_uTpbD_T\n",
    "        self.Y_up = Y_uTpbD_p\n",
    "        self.Y_lT = Y_lTpbD_T\n",
    "        self.Y_lp = Y_lTpbD_p\n",
    "\n",
    "        self.cof_PI1AD = cof_PI1AD\n",
    "        self.cof_PI2AD = cof_PI2AD\n",
    "        self.cof_pbN = cof_pbN\n",
    "        self.cof_TbN = cof_TbN\n",
    "        if (self.if_low_dirichlet):\n",
    "            self.cof_TbN = 0\n",
    "\n",
    "        # Initialize history of losses and global iteration counter\n",
    "        self.train_hist = []\n",
    "        self.val_hist = []\n",
    "        self.Tloss_hist = []\n",
    "        self.ploss_hist = []\n",
    "        self.kloss_hist = []\n",
    "        self.PI1lossAD_hist = []\n",
    "        self.PI2lossAD_hist = []\n",
    "        self.TpbDloss_hist = []\n",
    "        self.pbNloss_hist = []\n",
    "        self.TbNloss_hist = []\n",
    "        self.PI1lossAD_all_hist = []\n",
    "        self.PI2lossAD_all_hist = []\n",
    "        self.wPI1_hist = []\n",
    "        self.wPI2_hist = []\n",
    "        self.wTbN_hist = []\n",
    "        self.iter = 0\n",
    "\n",
    "        # Reference (true) data\n",
    "        self.T_true = T_true\n",
    "        self.p_true = p_true\n",
    "        self.k_true = k_true\n",
    "        \n",
    "        # Data for normalization\n",
    "        self.T_norm = T_for_normalize\n",
    "        self.p_norm = p_for_normalize\n",
    "        self.k_norm = k_for_normalize\n",
    "\n",
    "        # loss mode\n",
    "        self.lmode = lmode\n",
    "\n",
    "        # Parameters for interpolation\n",
    "        self.order = order_interpolate\n",
    "        self.reg = regularization_interpolate\n",
    "\n",
    "        # The type of floating-point numbers\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        # Weighting parameters for each loss function\n",
    "        self.wPI1 = 1.0\n",
    "        self.wPI2 = 1.0\n",
    "        self.wTbN = 1.0\n",
    "        self.gTPK = []\n",
    "        self.gPI1 = []\n",
    "        self.gPI2 = []\n",
    "        self.gTbN = []\n",
    "\n",
    "    def Coef_IF97_Eq15_Region1(self,i):\n",
    "        if i == 1:\n",
    "            Ii = 0; Ji = -2; ni = 0.14632971213167\n",
    "        elif i == 2:\n",
    "            Ii = 0; Ji = -1; ni = -0.84548187169114\n",
    "        elif i == 3:\n",
    "            Ii = 0; Ji = 0; ni = -0.37563603672040*10\n",
    "        elif i == 4:\n",
    "            Ii = 0; Ji = 1; ni = 0.33855169168385*10\n",
    "        elif i == 5:\n",
    "            Ii = 0; Ji = 2; ni = -0.95791963387872\n",
    "        elif i == 6:\n",
    "            Ii = 0; Ji = 3; ni = 0.15772038513228\n",
    "        elif i == 7:\n",
    "            Ii = 0; Ji = 4; ni = -0.16616417199501*10**(-1)\n",
    "        elif i == 8:\n",
    "            Ii = 0; Ji = 5; ni = 0.81214629983568*10**(-3)\n",
    "        elif i == 9:\n",
    "            Ii = 1; Ji = -9; ni = 0.28319080123804*10**(-3)\n",
    "        elif i == 10:\n",
    "            Ii = 1; Ji = -7; ni = -0.60706301565874*10**(-3)\n",
    "        elif i == 11:\n",
    "            Ii = 1; Ji = -1; ni = -0.18990068218419*10**(-1)\n",
    "        elif i == 12:\n",
    "            Ii = 1; Ji = 0; ni = -0.32529748770505*10**(-1)\n",
    "        elif i == 13:\n",
    "            Ii = 1; Ji = 1; ni = -0.21841717175414*10**(-1)\n",
    "        elif i == 14:\n",
    "            Ii = 1; Ji = 3; ni = -0.52838357969930*10**(-4)\n",
    "        elif i == 15:\n",
    "             Ii = 2; Ji = -3; ni = -0.47184321073267*10**(-3)\n",
    "        elif i == 16:\n",
    "            Ii = 2; Ji = 0; ni = -0.30001780793026*10**(-3)\n",
    "        elif i == 17:\n",
    "            Ii = 2; Ji = 1; ni = 0.47661393906987*10**(-4)\n",
    "        elif i == 18:\n",
    "            Ii = 2; Ji = 3; ni = -0.44141845330846*10**(-5)\n",
    "        elif i == 19:\n",
    "            Ii = 2; Ji = 17; ni = -0.72694996297594*10**(-15)\n",
    "        elif i == 20:\n",
    "            Ii = 3; Ji = -4; ni = -0.31679644845054*10**(-4)\n",
    "        elif i == 21:\n",
    "            Ii = 3; Ji = 0; ni = -0.28270797985312*10**(-5)\n",
    "        elif i == 22:\n",
    "            Ii = 3; Ji = 6; ni = -0.85205128120103*10**(-9)\n",
    "        elif i == 23:\n",
    "            Ii = 4; Ji = -5; ni = -0.22425281908000*10**(-5)\n",
    "        elif i == 24:\n",
    "            Ii = 4; Ji = -2; ni = -0.65171222895601*10**(-6)\n",
    "        elif i == 25:\n",
    "            Ii = 4; Ji = 10; ni = -0.14341729937924*10**(-12)\n",
    "        elif i == 26:\n",
    "            Ii = 5; Ji = -8; ni = -0.40516996860117*10**(-6)\n",
    "        elif i == 27:\n",
    "            Ii = 8; Ji = -11; ni = -0.12734301741641*10**(-8)\n",
    "        elif i == 28:\n",
    "            Ii = 8; Ji = -6; ni = -0.17424871230634*10**(-9)\n",
    "        elif i == 29:\n",
    "            Ii = 21; Ji = -29; ni = -0.68762131295531*10**(-18)\n",
    "        elif i == 30:\n",
    "            Ii = 23; Ji = -31; ni = 0.14478307828521*10**(-19)\n",
    "        elif i == 31:\n",
    "            Ii = 29; Ji = -38; ni = 0.26335781662795*10**(-22)\n",
    "        elif i == 32:\n",
    "            Ii = 30; Ji = -39; ni = -0.11947622640071*10**(-22)\n",
    "        elif i == 33:\n",
    "            Ii = 31; Ji = -40; ni = 0.18228094581404*10**(-23)\n",
    "        elif i == 34:\n",
    "            Ii = 32; Ji = -41; ni = -0.93537087292458*10**(-25)\n",
    "        return Ii, Ji, ni\n",
    "\n",
    "    # Specific volume at Region 1\n",
    "    def IF97_SpecificVol_Region1(self,Tdeg,pMPa):\n",
    "        gamma_pi = 0.0\n",
    "        R = 0.461526*10**(-3) # Specific gas constant [kJ/(g*K)]\n",
    "        TK = Tdeg + 273\n",
    "        tau = 1386/TK\n",
    "        ppi = pMPa/16.53\n",
    "        for i in range(1,35):\n",
    "            Ii, Ji, ni = self.Coef_IF97_Eq15_Region1(i)\n",
    "            gamma_pi = gamma_pi - ni*Ii*((7.1-ppi)**(Ii-1))*((tau-1.222)**Ji)\n",
    "        svol = ppi*gamma_pi*R*TK/pMPa\n",
    "        return svol\n",
    "    \n",
    "    # Region 1 Specific_isobaric_heatcapacity\n",
    "    def IF97_SpecificCp_Region1(self,Tdeg,pMPa):\n",
    "        gamma_tautau = 0.0\n",
    "        R = 0.461526 # Specific gas constant [kJ/(kg*K)]\n",
    "        TK = Tdeg + 273\n",
    "        tau = 1386/TK\n",
    "        ppi = pMPa/16.53\n",
    "        for i in range(1,35):\n",
    "            Ii, Ji, ni = self.Coef_IF97_Eq15_Region1(i)\n",
    "            gamma_tautau = gamma_tautau + ni*((7.1-ppi)**Ii)*Ji*(Ji-1)*(tau-1.222)**(Ji-2)\n",
    "        scp = -R*(tau**2)*gamma_tautau\n",
    "        return scp*10**3 # [J/(kg*K)]\n",
    "    \n",
    "    # Calculate density of water\n",
    "    def Densw_pred(self,Tdeg,pPa):\n",
    "        pMPa = pPa/(10**6)\n",
    "        Densw = 1.0/self.IF97_SpecificVol_Region1(Tdeg,pMPa)\n",
    "        return Densw\n",
    "    \n",
    "    # Calculate heat capacity\n",
    "    def HCw_pred(self,Tdeg,pPa):\n",
    "        pMPa = pPa/(10**6)\n",
    "        HCw_calc = self.IF97_SpecificCp_Region1(Tdeg,pMPa)\n",
    "        return HCw_calc\n",
    "    \n",
    "    def Viscow_pred(self,Tdeg,dens):\n",
    "        TK = Tdeg + 273\n",
    "        That = TK / 647.096\n",
    "        cof0 = 1.67752 / That**0\n",
    "        cof1 = 2.20462 / That**1\n",
    "        cof2 = 0.6366564 / That**2\n",
    "        cof3 = -0.241605 / That**3\n",
    "        cof = cof0 + cof1 + cof2 + cof3\n",
    "        myu0 = 100.0 * tf.math.sqrt(That) / cof\n",
    "        H0 = [5.20094*10**(-1), 2.22531*10**(-1), -2.81378*10**(-1), 1.61913*10**(-1), -3.25372*10**(-2), 0.0, 0.0]\n",
    "        H1 = [8.50895*10**(-2), 9.99115*10**(-1), -9.06851*10**(-1), 2.57399*10**(-1), 0.0, 0.0, 0.0]\n",
    "        H2 = [-1.08374, 1.88797, -7.72479*10**(-1), 0.0, 0.0, 0.0, 0.0]\n",
    "        H3 = [-2.89555*10**(-1), 1.26613, -4.89837*10**(-1), 0.0, 6.98452*10**(-2), 0.0, -4.35673*10**(-3)]\n",
    "        H4 = [0.0, 0.0, -2.57040*10**(-1), 0.0, 0.0, 8.72102*10**(-3), 0.0]\n",
    "        H5 = [0.0, 1.20573*10**(-1), 0.0, 0.0, 0.0, 0.0, -5.93264*10**(-4)]\n",
    "        H = [H0, H1, H2, H3, H4, H5]\n",
    "        rouhat = dens / 322.0\n",
    "        myu1tp = 0.0\n",
    "        for ii in range(0,6):\n",
    "            Hsum = 0.0\n",
    "            for jj in range(0,7):\n",
    "                Hsum += H[ii][jj] * (rouhat-1)**jj\n",
    "            myu1tp += ((1/That-1)**ii) * Hsum\n",
    "        myu1 = tf.math.exp(rouhat*myu1tp)\n",
    "        Visw_calc = myu0 * myu1 * 10**(-6)\n",
    "        return Visw_calc\n",
    "\n",
    "    def denormalize(self,xx,xxdata):\n",
    "        return xx*(np.max(xxdata)-np.min(xxdata)) + np.min(xxdata)\n",
    "\n",
    "    def normalize(self,xx,xxdata):\n",
    "        datamax = tf.reduce_max(xxdata)\n",
    "        datamin = tf.reduce_min(xxdata)\n",
    "        return (xx - datamin) / (datamax - datamin)\n",
    "\n",
    "    def reshape_and_rescale(\n",
    "            self, var_grid: tf.Tensor,\n",
    "            var: str) -> tf.Tensor:\n",
    "\n",
    "        # The 3D grid data are reordered into a single column.\n",
    "        var_ansatz = rcc.rearrange_grid_to_list(var_grid)\n",
    "\n",
    "        # # Denormalize the quantities\n",
    "        if var=='k': # permeability\n",
    "            var_denm = self.denormalize(var_ansatz, self.k_norm)\n",
    "        elif (var == 'T'): # temperature\n",
    "            var_denm = self.denormalize(var_ansatz, self.T_norm)\n",
    "        elif (var == 'p'): # pressure\n",
    "            var_denm = self.denormalize(var_ansatz, self.p_norm)\n",
    "\n",
    "        return var_denm\n",
    "\n",
    "    def extract_points(self,\n",
    "            in_tensor: tf.Tensor, grid_shape: tuple, var: Enum,\n",
    "            model_input_grid: tf.Tensor,\n",
    "            point_indices: tf.Tensor) -> tf.Tensor:\n",
    "\n",
    "        # Re-organize the data in a 3D grid format.\n",
    "        arr_grid = rcc.rearrange_list_to_grid(arr_list=in_tensor, grid_shape=grid_shape)\n",
    "        # The data at the collocation-points are extracted and reordered into a single column.\n",
    "        arr_extracted = tf.gather_nd(tf.squeeze(arr_grid, axis=0), point_indices)\n",
    "\n",
    "        return arr_extracted\n",
    "    \n",
    "    def get_mass_loss(self):\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape1:\n",
    "            # Watch variables with this GradientTape\n",
    "            tape1.watch(self._model_input_grid)\n",
    "            with tf.GradientTape(persistent=True) as tape2:\n",
    "                # Watch variables with this GradientTape\n",
    "                tape2.watch(self._model_input_grid)\n",
    "            \n",
    "                # Compute quantities by the PINN \n",
    "                # input shape = (1, N_x1, N_x2, N_x3, 3)\n",
    "                # each output shape = (1, N_x1, N_x2, N_x3, 1)\n",
    "                T_grid, p_grid, k_grid = self.model(self._model_input_grid)\n",
    "\n",
    "                # gathered shape = (N_f, 1)\n",
    "                pdenm = self.denormalize(p_grid, p_star)\n",
    "\n",
    "            # Derivatives are taken from each variable in the entire input grid.\n",
    "            p_x_grid = tape2.gradient(pdenm, self._model_input_grid)  # shape = (1, N_x1, N_x2, N_x3, 3)\n",
    "            p_x1 = p_x_grid[:,:,:,:,0:1]\n",
    "            p_x2 = p_x_grid[:,:,:,:,1:2]\n",
    "            p_x3 = p_x_grid[:,:,:,:,2:3]\n",
    "            Tdenm = self.denormalize(T_grid, T_star)\n",
    "            kdenm = self.denormalize(k_grid, k_star)\n",
    "            dens = self.Densw_pred(Tdenm, pdenm)\n",
    "            visc = self.Viscow_pred(Tdenm, dens)\n",
    "            densg = dens * 9.8\n",
    "            ddv = tf.math.divide(dens,visc)\n",
    "            pf_x1 = tf.math.multiply(ddv, p_x1)\n",
    "            pf_x3 = tf.math.multiply(ddv, p_x3)\n",
    "            pf_x2tp = tf.math.subtract(p_x2,densg)\n",
    "            pf_x2 = tf.math.multiply(ddv, pf_x2tp)\n",
    "            g1 = tf.math.multiply(10**kdenm, pf_x1)\n",
    "            g2 = tf.math.multiply(10**kdenm, pf_x2)\n",
    "            g3 = tf.math.multiply(10**kdenm, pf_x3)\n",
    "        \n",
    "        f1tp = tape1.gradient(g1, self._model_input_grid)  # shape = (1, N_x1, N_x2, N_x3, 1)\n",
    "        f2tp = tape1.gradient(g2, self._model_input_grid)\n",
    "        f3tp = tape1.gradient(g3, self._model_input_grid)\n",
    "        posf1 = tf.math.is_finite(f1tp)\n",
    "        posf2 = tf.math.is_finite(f2tp)\n",
    "        posf3 = tf.math.is_finite(f3tp)\n",
    "        f1 = tf.where(posf1,f1tp,[10**5])\n",
    "        f2 = tf.where(posf2,f2tp,[10**5])\n",
    "        f3 = tf.where(posf3,f3tp,[10**5])\n",
    "        \n",
    "        del tape2\n",
    "        del tape1\n",
    "        \n",
    "        return self.func_ms(f1, f2, f3)\n",
    "\n",
    "    def get_energy_loss(self):\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape1:\n",
    "            tape1.watch(self._model_input_grid)\n",
    "            with tf.GradientTape(persistent=True) as tape2:\n",
    "                tape2.watch(self._model_input_grid)\n",
    "\n",
    "                # Compute quantities by the PINN \n",
    "                # input shape = (1, N_x1, N_x2, N_x3, 3)\n",
    "                # each output shape = (1, N_x1, N_x2, N_x3, 1)\n",
    "                T_grid, p_grid, k_grid = self.model(self._model_input_grid)\n",
    "                pdenm = self.denormalize(p_grid, p_star)\n",
    "                Tdenm = self.denormalize(T_grid, T_star)\n",
    "\n",
    "            p_x_grid = tape2.gradient(pdenm, self._model_input_grid)  # shape = (1, N_x1, N_x2, N_x3, 1)\n",
    "            p_x1 = p_x_grid[:,:,:,:,0:1]\n",
    "            p_x2 = p_x_grid[:,:,:,:,1:2]\n",
    "            p_x3 = p_x_grid[:,:,:,:,2:3]    \n",
    "            T_x_grid = tape2.gradient(Tdenm, self._model_input_grid)  # shape = (1, N_x1, N_x2, N_x3, 1)\n",
    "            T_x1 = T_x_grid[:,:,:,:,0:1]\n",
    "            T_x2 = T_x_grid[:,:,:,:,1:2]\n",
    "            T_x3 = T_x_grid[:,:,:,:,2:3]\n",
    "\n",
    "            kdenm = self.denormalize(k_grid, k_star)\n",
    "            dens = self.Densw_pred(Tdenm, pdenm)\n",
    "            visc = self.Viscow_pred(Tdenm, dens)\n",
    "            hcw = self.HCw_pred(Tdenm, pdenm)\n",
    "            densg = dens * 9.8\n",
    "            ddw_tp1 = tf.math.divide(dens,visc)\n",
    "            ddw_tp2 = tf.math.multiply(hcw,ddw_tp1)\n",
    "            ddw = tf.math.multiply(Tdenm,ddw_tp2)\n",
    "            pf_x1 = tf.math.multiply(ddw, p_x1)\n",
    "            pf_x3 = tf.math.multiply(ddw, p_x3)\n",
    "            pf_x2tp = tf.math.subtract(p_x2,densg)\n",
    "            pf_x2 = tf.math.multiply(ddw, pf_x2tp)\n",
    "            g1 = tf.math.multiply(10**kdenm, pf_x1)\n",
    "            g2 = tf.math.multiply(10**kdenm, pf_x2)\n",
    "            g3 = tf.math.multiply(10**kdenm, pf_x3)\n",
    "        \n",
    "        f_x1tp_grid = tape1.gradient(g1, self._model_input_grid)\n",
    "        f_x2tp_grid = tape1.gradient(g2, self._model_input_grid)\n",
    "        f_x3tp_grid = tape1.gradient(g3, self._model_input_grid)\n",
    "        posf1 = tf.math.is_finite(f_x1tp_grid)\n",
    "        posf2 = tf.math.is_finite(f_x2tp_grid)\n",
    "        posf3 = tf.math.is_finite(f_x3tp_grid)\n",
    "        f_x1x1 = tf.where(posf1,f_x1tp_grid,[10**5])\n",
    "        f_x2x2 = tf.where(posf2,f_x2tp_grid,[10**5])\n",
    "        f_x3x3 = tf.where(posf3,f_x3tp_grid,[10**5])\n",
    "                \n",
    "        T_x1x1 = tape1.gradient(T_x1, self._model_input_grid)\n",
    "        T_x2x2 = tape1.gradient(T_x2, self._model_input_grid)\n",
    "        T_x3x3 = tape1.gradient(T_x3, self._model_input_grid)\n",
    "        \n",
    "        del tape2\n",
    "        del tape1\n",
    "        \n",
    "        return self.func_en(f_x1x1, f_x2x2, f_x3x3, T_x1x1, T_x2x2, T_x3x3)\n",
    "    \n",
    "    \n",
    "    def func_ms(self, fl1, fl2, fl3):\n",
    "        \"\"\"Residual of the PDE\"\"\"\n",
    "        return fl1 + fl2 + fl3\n",
    "\n",
    "    def func_en(self, fl_x1x1, fl_x2x2, fl_x3x3,\n",
    "               T_x1x1, T_x2x2, T_x3x3):\n",
    "        \"\"\"Residual of the PDE\"\"\"\n",
    "        return (fl_x1x1 + fl_x2x2 + fl_x3x3\n",
    "                + Lambda * (T_x1x1 + T_x2x2 + T_x3x3))\n",
    "    \n",
    "    def get_TpbD(self):\n",
    "\n",
    "        T_grid, p_grid, _ = self.model(self._model_input_grid)\n",
    "\n",
    "        # The data stored in a three-dimensional grid is rearranged into a single list and converted into physical quantities.\n",
    "        Tdenm = self.reshape_and_rescale(T_grid, var='T')\n",
    "        pdenm = self.reshape_and_rescale(p_grid, var='p')\n",
    "        \n",
    "        # Extract data at the upper surface boundary \n",
    "        # gathered shape = (N_up, 1)\n",
    "        grid_shape = T_grid.shape\n",
    "        utt_denm = self.extract_points(\n",
    "            in_tensor=Tdenm, grid_shape=grid_shape, var=VariableType.T,\n",
    "            model_input_grid=self._model_input_grid,\n",
    "            point_indices=self._model_input_grid_spec.up_indices)\n",
    "        upp_denm = self.extract_points(\n",
    "            in_tensor=pdenm, grid_shape=grid_shape, var=VariableType.p,\n",
    "            model_input_grid=self._model_input_grid,\n",
    "            point_indices=self._model_input_grid_spec.up_indices)\n",
    "\n",
    "        # Extract data at the bottom boundary\n",
    "        grid_shape = T_grid.shape\n",
    "        ltt_denm = self.extract_points(\n",
    "            in_tensor=Tdenm, grid_shape=grid_shape, var=VariableType.T,\n",
    "            model_input_grid=self._model_input_grid,\n",
    "            point_indices=self._model_input_grid_spec.low_indices)\n",
    "        lpp_denm = self.extract_points(\n",
    "            in_tensor=pdenm, grid_shape=grid_shape, var=VariableType.p,\n",
    "            model_input_grid=self._model_input_grid,\n",
    "            point_indices=self._model_input_grid_spec.low_indices)\n",
    "\n",
    "        # Normalization\n",
    "        N_up = self._model_input_grid_spec.up_indices.shape[0]\n",
    "        utt = tf.reshape(self.normalize(utt_denm, self.T_norm), [1, N_up, 1])\n",
    "        upp = tf.reshape(self.normalize(upp_denm, self.p_norm), [1, N_up, 1])\n",
    "\n",
    "        N_low = self._model_input_grid_spec.low_indices.shape[0]\n",
    "        ltt = tf.reshape(self.normalize(ltt_denm, self.T_norm), [1, N_low, 1])\n",
    "        lpp = tf.reshape(self.normalize(lpp_denm, self.p_norm), [1, N_low, 1])\n",
    "\n",
    "        return utt, upp, ltt, lpp\n",
    "\n",
    "    def get_TbN(self):\n",
    "\n",
    "        with tf.GradientTape() as tape11, \\\n",
    "            tf.GradientTape() as tape12, \\\n",
    "                tf.GradientTape() as tape13:\n",
    "            tape11.watch(self._model_input_grid)\n",
    "            tape12.watch(self._model_input_grid)\n",
    "            tape13.watch(self._model_input_grid)\n",
    "\n",
    "            # Prediction by the PINN\n",
    "            # input shape = (1, N_x1, N_x2, N_x3, 3)\n",
    "            # each output shape = (1, N_x1, N_x2, N_x3, 1)\n",
    "            new_grid = self._model_input_grid\n",
    "            T_grid, p_grid, _ = self.model(new_grid)\n",
    "\n",
    "            # The data stored in a three-dimensional grid is rearranged into a single list and converted into physical quantities.\n",
    "            Tdenm = self.reshape_and_rescale(T_grid, var='T')\n",
    "            pdenm = self.reshape_and_rescale(p_grid, var='p')\n",
    "\n",
    "            grid_shape = T_grid.shape\n",
    "            Tdenm = self.extract_points(\n",
    "                in_tensor=Tdenm, grid_shape=grid_shape, var=VariableType.T,\n",
    "                model_input_grid=self._model_input_grid,\n",
    "                point_indices=self._model_input_grid_spec.low_indices)\n",
    "\n",
    "        gT_1_grid = tape11.gradient(Tdenm, self._model_input_grid)\n",
    "        gT_1 = tf.gather_nd(tf.squeeze(gT_1_grid, axis=0), self._model_input_grid_spec.low_indices)[:, 0:1]\n",
    "        gT_2_grid = tape12.gradient(Tdenm, self._model_input_grid)\n",
    "        gT_2 = tf.gather_nd(tf.squeeze(gT_2_grid, axis=0), self._model_input_grid_spec.low_indices)[:, 1:2]\n",
    "        gT_3_grid = tape13.gradient(Tdenm, self._model_input_grid)\n",
    "        gT_3 = tf.gather_nd(tf.squeeze(gT_3_grid, axis=0), self._model_input_grid_spec.low_indices)[:, 2:3]\n",
    "\n",
    "        gT = gT_1*self.nvecT[:,0:1] + gT_2*self.nvecT[:,1:2]+ gT_3*self.nvecT[:,2:3]\n",
    "\n",
    "        return gT\n",
    "\n",
    "    def calculate_tpk_loss(self, X,\n",
    "                            X_wells, T_wells, p_wells, k_wells,\n",
    "                            well_neighbor_indices):\n",
    "\n",
    "        # Calculate loss function for T, p, k.\n",
    "        N = X.shape[1]\n",
    "        x = tf.reshape(X, [1, N, 3])\n",
    "        T_grid, p_grid, k_grid = self.model(self._model_input_grid)\n",
    "\n",
    "        # The data stored in a three-dimensional grid is rearranged into a single list and converted into physical quantities.\n",
    "        Tdenm = self.reshape_and_rescale(T_grid, var='T')\n",
    "        pdenm = self.reshape_and_rescale(p_grid, var='p')\n",
    "        kdenm = self.reshape_and_rescale(k_grid, var='k')\n",
    "        \n",
    "        # Extracts data from points near the wells\n",
    "        grid_well_neighbor_indices = (\n",
    "            tf.gather(\n",
    "                tf.transpose(\n",
    "                    tf.unravel_index(tf.squeeze(well_neighbor_indices, axis=0),\n",
    "                                     dims=(self._model_input_grid_spec.N_x3,\n",
    "                                           self._model_input_grid_spec.N_x2,\n",
    "                                           self._model_input_grid_spec.N_x1))),\n",
    "                indices=[2, 1, 0],\n",
    "                axis=1)\n",
    "        )\n",
    "\n",
    "        grid_shape = T_grid.shape\n",
    "        Tdenm = self.extract_points(\n",
    "            in_tensor=Tdenm, grid_shape=grid_shape, var=VariableType.T,\n",
    "            model_input_grid=self._model_input_grid,\n",
    "            point_indices=grid_well_neighbor_indices)\n",
    "        pdenm = self.extract_points(\n",
    "            in_tensor=pdenm, grid_shape=grid_shape, var=VariableType.p,\n",
    "            model_input_grid=self._model_input_grid,\n",
    "            point_indices=grid_well_neighbor_indices)\n",
    "        kdenm = self.extract_points(\n",
    "            in_tensor=kdenm, grid_shape=grid_shape, var=VariableType.k,\n",
    "            model_input_grid=self._model_input_grid,\n",
    "            point_indices=grid_well_neighbor_indices)\n",
    "\n",
    "        # Normalization\n",
    "        T_pred = tf.reshape(self.normalize(Tdenm, self.T_norm), [1, N, 1])\n",
    "        p_pred = tf.reshape(self.normalize(pdenm, self.p_norm), [1, N, 1])\n",
    "        k_pred = tf.reshape(self.normalize(kdenm, self.k_norm), [1, N, 1])\n",
    "\n",
    "        # Obtain T, P, and K at wells\n",
    "        T_interp = tfa.image.interpolate_spline(x, T_pred, X_wells, self.order, regularization_weight=self.reg)\n",
    "        p_interp = tfa.image.interpolate_spline(x, p_pred, X_wells, self.order, regularization_weight=self.reg)\n",
    "        k_interp = tfa.image.interpolate_spline(x, k_pred, X_wells, self.order, regularization_weight=self.reg)\n",
    "\n",
    "        Tloss = tf.reduce_mean(tf.square(T_interp - T_wells))\n",
    "        ploss = tf.reduce_mean(tf.square(p_interp - p_wells))\n",
    "        kloss = tf.reduce_mean(tf.square(k_interp - k_wells))\n",
    "\n",
    "        return Tloss, ploss, kloss\n",
    "\n",
    "    def loss_func(self, X,\n",
    "                Y_pbN, Y_TbN,\n",
    "                X_wells, T_wells, p_wells, k_wells,\n",
    "                well_neighbor_indices, dstype):\n",
    "\n",
    "        if self.lmode == 1 or self.lmode == 0:\n",
    "\n",
    "            # Compute phi_r (physics-informed constraint)          \n",
    "            lm_AD = self.get_mass_loss()\n",
    "            le_AD = self.get_energy_loss()\n",
    "            PI1loss_AD = tf.reduce_mean(tf.square(lm_AD))\n",
    "            PI2loss_AD = tf.reduce_mean(tf.square(le_AD))\n",
    "\n",
    "            # Compute boundary condition\n",
    "            uTpbD_T_pred, uTpbD_p_pred, lTpbD_T_pred, lTpbD_p_pred= self.get_TpbD()\n",
    "\n",
    "            TbN_pred = self.get_TbN()\n",
    "\n",
    "            # Loss of boudary condition\n",
    "            uTpbDloss_T = tf.reduce_mean(tf.square(self.Y_uT - uTpbD_T_pred))\n",
    "            uTpbDloss_p = tf.reduce_mean(tf.square(self.Y_up - uTpbD_p_pred))\n",
    "            lTpbDloss_T = tf.reduce_mean(tf.square(self.Y_lT - lTpbD_T_pred))\n",
    "            lTpbDloss_p = tf.reduce_mean(tf.square(self.Y_lp - lTpbD_p_pred))\n",
    "            uTpbDloss = tf.add(uTpbDloss_T, uTpbDloss_p)\n",
    "            lTpbDloss = tf.add(lTpbDloss_T, lTpbDloss_p)\n",
    "            if (self.if_low_dirichlet):\n",
    "                TpbDloss = tf.add(uTpbDloss, lTpbDloss)\n",
    "                TbNloss =  tf.constant(0.0, self.dtype)\n",
    "            else:\n",
    "                TpbDloss = uTpbDloss\n",
    "                TbNloss = tf.reduce_mean(tf.square(Y_TbN - TbN_pred))\n",
    "\n",
    "        else:\n",
    "\n",
    "            PI1loss_FD = tf.constant(0.0, self.dtype)\n",
    "            PI2loss_FD = tf.constant(0.0, self.dtype)\n",
    "            PI1loss_AD = tf.constant(0.0, self.dtype)\n",
    "            PI2loss_AD = tf.constant(0.0, self.dtype)\n",
    "            TpbDloss = tf.constant(0.0, self.dtype)\n",
    "            TbNloss =  tf.constant(0.0, self.dtype)\n",
    "\n",
    "        # If the data set is training or validation, the loss of T, p, and k is calculated.\n",
    "        if (dstype != DataSet.ALL):\n",
    "            Tloss, ploss, kloss \\\n",
    "                = self.calculate_tpk_loss(X,\n",
    "                                            X_wells, T_wells, p_wells, k_wells,\n",
    "                                            well_neighbor_indices)\n",
    "        else:\n",
    "            Tloss = tf.constant(0, dtype = self.dtype)\n",
    "            ploss = tf.constant(0, dtype = self.dtype)\n",
    "            kloss = tf.constant(0, dtype = self.dtype)\n",
    "\n",
    "        # Sum up\n",
    "        if self.lmode <= 0:\n",
    "            loss = Tloss + ploss + kloss        \n",
    "        else:\n",
    "            loss = Tloss + ploss + kloss + self.wPI1*PI1loss_AD*self.cof_PI1AD + self.wPI2*PI2loss_AD*self.cof_PI2AD + TpbDloss + self.wTbN*TbNloss*self.cof_TbN\n",
    "\n",
    "        return loss, Tloss, ploss, kloss, PI1loss_AD, PI2loss_AD, TpbDloss, TbNloss\n",
    "\n",
    "    def get_grad(self, X,\n",
    "                Y_pbN, Y_TbN,\n",
    "                X_wells, T_wells, p_wells, k_wells,\n",
    "                well_neighbor_indices, dstype, nan_to_zero=True):\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape, \\\n",
    "            tf.GradientTape(persistent=True) as tapeT, \\\n",
    "                tf.GradientTape(persistent=True) as tapeP, \\\n",
    "             tf.GradientTape(persistent=True) as tapeK, \\\n",
    "            tf.GradientTape(persistent=True) as tapePI1, \\\n",
    "            tf.GradientTape(persistent=True) as tapePI2, \\\n",
    "            tf.GradientTape(persistent=True) as tapeTbN:\n",
    "            #\n",
    "            tape.watch(self.model.trainable_variables)\n",
    "            tapeT.watch(self.model.trainable_variables)\n",
    "            tapeP.watch(self.model.trainable_variables)\n",
    "            tapeK.watch(self.model.trainable_variables)\n",
    "            tapePI1.watch(self.model.trainable_variables)\n",
    "            tapePI2.watch(self.model.trainable_variables)\n",
    "            tapeTbN.watch(self.model.trainable_variables)\n",
    "            loss,Tloss,ploss,kloss,PI1loss_AD,PI2loss_AD,TpbDloss,TbNloss\\\n",
    "                = self.loss_func(X,\n",
    "                            Y_pbN, Y_TbN,\n",
    "                            X_wells, T_wells, p_wells, k_wells,\n",
    "                            well_neighbor_indices, dstype)\n",
    "\n",
    "        g = tape.gradient(loss, self.model.trainable_variables)\n",
    "        if self.lmode < 0:\n",
    "            self.wPI1 = 1.0\n",
    "            self.wPI2 = 1.0\n",
    "            self.wTbN = 1.0\n",
    "            gradT = tapeT.gradient(Tloss, self.model.trainable_variables)\n",
    "            gradP = tapeP.gradient(ploss, self.model.trainable_variables)\n",
    "            gradK = tapeK.gradient(kloss, self.model.trainable_variables)\n",
    "            gradTPK_max_list = tf.constant([0.0], self.dtype)\n",
    "            for gradT_elem,gradP_elem,gradK_elem in zip(gradT,gradP,gradK):\n",
    "                if gradT_elem is None:\n",
    "                    is_gradT_nan = True\n",
    "                else:\n",
    "                    is_gradT_nan = tf.reduce_any(tf.math.is_nan(gradT_elem))\n",
    "                if gradP_elem is None:\n",
    "                    is_gradP_nan = True\n",
    "                else:\n",
    "                    is_gradP_nan = tf.reduce_any(tf.math.is_nan(gradP_elem))\n",
    "                if gradK_elem is None:\n",
    "                    is_gradK_nan = True\n",
    "                else:\n",
    "                    is_gradK_nan = tf.reduce_any(tf.math.is_nan(gradK_elem))\n",
    "                if not is_gradT_nan:\n",
    "                    gradTPK_max_list = tf.concat([gradTPK_max_list, [tf.math.reduce_mean(tf.abs(gradT_elem))]], axis=0)\n",
    "                if not is_gradP_nan:\n",
    "                    gradTPK_max_list = tf.concat([gradTPK_max_list, [tf.math.reduce_mean(tf.abs(gradP_elem))]], axis=0)\n",
    "                if not is_gradK_nan:\n",
    "                    gradTPK_max_list = tf.concat([gradTPK_max_list, [tf.math.reduce_mean(tf.abs(gradK_elem))]], axis=0)\n",
    "            gradTPK_max = tf.math.reduce_max(gradTPK_max_list)\n",
    "        elif self.iter % 100 == 0:\n",
    "            gradT = tapeT.gradient(Tloss, self.model.trainable_variables)\n",
    "            gradP = tapeP.gradient(ploss, self.model.trainable_variables)\n",
    "            gradK = tapeK.gradient(kloss, self.model.trainable_variables)\n",
    "            gradPI1 = tapePI1.gradient(PI1loss_AD, self.model.trainable_variables)\n",
    "            gradPI2 = tapePI2.gradient(PI2loss_AD, self.model.trainable_variables)\n",
    "            gradTbN = tapeTbN.gradient(TbNloss, self.model.trainable_variables)\n",
    "            gTPK_max_list = tf.constant([0.0], self.dtype)\n",
    "            gPI1_mean_list = tf.constant([], self.dtype)\n",
    "            gPI2_mean_list = tf.constant([], self.dtype)\n",
    "            gTbN_mean_list = tf.constant([], self.dtype)\n",
    "            for gT_el,gP_el,gK_el,gPI1_el,gPI2_el,gTbN_el in zip(gradT,gradP,gradK,gradPI1,gradPI2,gradTbN):\n",
    "                if gT_el is None:\n",
    "                    is_gradT_nan = True\n",
    "                else:\n",
    "                    is_gradT_nan = tf.reduce_any(tf.math.is_nan(gT_el))\n",
    "                if gP_el is None:\n",
    "                    is_gradP_nan = True\n",
    "                else:\n",
    "                    is_gradP_nan = tf.reduce_any(tf.math.is_nan(gP_el))\n",
    "                if gK_el is None:\n",
    "                    is_gradK_nan = True\n",
    "                else:\n",
    "                    is_gradK_nan = tf.reduce_any(tf.math.is_nan(gK_el))\n",
    "                if gPI1_el is None:\n",
    "                    is_gradPI1_nan = True\n",
    "                else:\n",
    "                    is_gradPI1_nan = tf.reduce_any(tf.math.is_nan(gPI1_el))\n",
    "                if gPI2_el is None:\n",
    "                    is_gradPI2_nan = True\n",
    "                else:\n",
    "                    is_gradPI2_nan = tf.reduce_any(tf.math.is_nan(gPI2_el))\n",
    "                if gTbN_el is None:\n",
    "                    is_gradTbN_nan = True\n",
    "                else:\n",
    "                    is_gradTbN_nan = tf.reduce_any(tf.math.is_nan(gTbN_el))\n",
    "                if not is_gradT_nan:\n",
    "                    gTPK_max_list = tf.concat([gTPK_max_list, [tf.math.reduce_mean(tf.abs(gT_el))]], axis=0)\n",
    "                if not is_gradP_nan:\n",
    "                    gTPK_max_list = tf.concat([gTPK_max_list, [tf.math.reduce_mean(tf.abs(gP_el))]], axis=0)\n",
    "                if not is_gradK_nan:\n",
    "                    gTPK_max_list = tf.concat([gTPK_max_list, [tf.math.reduce_mean(tf.abs(gK_el))]], axis=0)\n",
    "                if not is_gradPI1_nan:\n",
    "                    gPI1_mean_list = tf.concat([gPI1_mean_list, [tf.math.reduce_max(tf.abs(gPI1_el))]], axis=0)\n",
    "                if not is_gradPI2_nan:\n",
    "                    gPI2_mean_list = tf.concat([gPI2_mean_list, [tf.math.reduce_max(tf.abs(gPI2_el))]], axis=0)\n",
    "                if not is_gradTbN_nan:\n",
    "                    gTbN_mean_list = tf.concat([gTbN_mean_list, [tf.math.reduce_max(tf.abs(gTbN_el))]], axis=0)\n",
    "            gTPK_max = tf.math.reduce_mean(gTPK_max_list)\n",
    "            gPI1_mean = self.cof_PI1AD*tf.math.reduce_max(gPI1_mean_list)\n",
    "            gPI2_mean = self.cof_PI2AD*tf.math.reduce_max(gPI2_mean_list)\n",
    "            gTbN_mean = self.cof_TbN*tf.math.reduce_max(gTbN_mean_list)\n",
    "            if not gTPK_max == 0.0:\n",
    "                if gPI1_mean is not None and not tf.reduce_any(tf.math.is_nan(gPI1_mean)):\n",
    "                    wPI1_hat = gTPK_max / gPI1_mean\n",
    "                    if wPI1_hat is not None and not tf.reduce_any(tf.math.is_nan(wPI1_hat)):\n",
    "                        self.wPI1 = 0.6 * self.wPI1 + 0.4 * wPI1_hat\n",
    "                if gPI2_mean is not None and not tf.reduce_any(tf.math.is_nan(gPI2_mean)):\n",
    "                    wPI2_hat = gTPK_max / gPI2_mean\n",
    "                    if wPI2_hat is not None and not tf.reduce_any(tf.math.is_nan(wPI2_hat)):\n",
    "                        self.wPI2 = 0.6 * self.wPI2 + 0.4 * wPI2_hat\n",
    "                if gTbN_mean is not None and not tf.reduce_any(tf.math.is_nan(gTbN_mean)):\n",
    "                    wTbN_hat = gTPK_max / gTbN_mean\n",
    "                    if wTbN_hat is not None and not tf.reduce_any(tf.math.is_nan(wTbN_hat)):\n",
    "                        self.wTbN = 0.6 * self.wTbN + 0.4 * wTbN_hat\n",
    "                        \n",
    "            if self.iter % 2500 == 0 and self.iter != 0:\n",
    "                for gT_el,gP_el,gK_el,gPI1_el,gPI2_el,gTbN_el in zip(gradT,gradP,gradK,gradPI1,gradPI2,gradTbN):\n",
    "                    if gT_el is None:\n",
    "                        is_gradT_nan = True\n",
    "                    else:\n",
    "                        is_gradT_nan = tf.reduce_any(tf.math.is_nan(gT_el))\n",
    "                    if gP_el is None:\n",
    "                        is_gradP_nan = True\n",
    "                    else:\n",
    "                        is_gradP_nan = tf.reduce_any(tf.math.is_nan(gP_el))\n",
    "                    if gK_el is None:\n",
    "                        is_gradK_nan = True\n",
    "                    else:\n",
    "                        is_gradK_nan = tf.reduce_any(tf.math.is_nan(gK_el))\n",
    "                    if gPI1_el is None:\n",
    "                        is_gradPI1_nan = True\n",
    "                    else:\n",
    "                        is_gradPI1_nan = tf.reduce_any(tf.math.is_nan(gPI1_el))\n",
    "                    if gPI2_el is None:\n",
    "                        is_gradPI2_nan = True\n",
    "                    else:\n",
    "                        is_gradPI2_nan = tf.reduce_any(tf.math.is_nan(gPI2_el))\n",
    "                    if gTbN_el is None:\n",
    "                        is_gradTbN_nan = True\n",
    "                    else:\n",
    "                        is_gradTbN_nan = tf.reduce_any(tf.math.is_nan(gTbN_el))\n",
    "                    if not is_gradT_nan:\n",
    "                        gTel_flatten = self.flatten_list(tf.reshape(tf.squeeze(gT_el), [-1]).numpy().tolist())\n",
    "                        self.gTPK.append(gTel_flatten)\n",
    "                    if not is_gradP_nan:\n",
    "                        gPel_flatten = self.flatten_list(tf.reshape(tf.squeeze(gP_el), [-1]).numpy().tolist())\n",
    "                        self.gTPK.append(gPel_flatten)\n",
    "                    if not is_gradK_nan:\n",
    "                        gKel_flatten = self.flatten_list(tf.reshape(tf.squeeze(gK_el), [-1]).numpy().tolist())\n",
    "                        self.gTPK.append(gKel_flatten)\n",
    "                    if not is_gradPI1_nan:\n",
    "                        gPI1el_flatten = self.flatten_list(tf.reshape(tf.squeeze(gPI1_el), [-1]).numpy().tolist())\n",
    "                        self.gPI1.append(gPI1el_flatten)\n",
    "                    if not is_gradPI2_nan:\n",
    "                        gPI2el_flatten = self.flatten_list(tf.reshape(tf.squeeze(gPI2_el), [-1]).numpy().tolist())\n",
    "                        self.gPI2.append(gPI2el_flatten)\n",
    "                    if not is_gradTbN_nan:\n",
    "                        gTbNel_flatten = self.flatten_list(tf.reshape(tf.squeeze(gTbN_el), [-1]).numpy().tolist())\n",
    "                        self.gTbN.append(gTbNel_flatten)\n",
    "                \n",
    "        if nan_to_zero:\n",
    "            ind = 0\n",
    "            for var, grad in zip(self.model.trainable_variables, g):\n",
    "                if grad is not None:\n",
    "                    max_grad = tf.reduce_max(tf.abs(grad))\n",
    "                    is_grad_nan = tf.reduce_any(tf.math.is_nan(grad))\n",
    "                    if is_grad_nan:\n",
    "                        g[ind] = tf.zeros_like(grad)\n",
    "                ind += 1\n",
    "                \n",
    "        del tape,tapeT,tapeP,tapeK,tapePI1,tapePI2,tapeTbN\n",
    "\n",
    "        return loss, Tloss, ploss, kloss, PI1loss_AD, PI2loss_AD, TpbDloss, TbNloss, g\n",
    "\n",
    "    def solve_with_TFoptimizer(self, optimizer, train_ds, val_ds, all_ds, N=1001):\n",
    "        \"\"\"This method performs a gradient descent type optimization.\"\"\"\n",
    "\n",
    "        self.max_iteration = N\n",
    "\n",
    "        #@tf.function\n",
    "        def train_step():\n",
    "            loss,Tloss,ploss,kloss,PI1loss_AD,PI2loss_AD,TpbDloss,TbNloss,grad_theta \\\n",
    "                = self.get_grad(X,\n",
    "                                Y_pbN, Y_TbN,\n",
    "                                X_wells, T_wells, p_wells, k_wells,\n",
    "                                well_neighbor_indices, dstype)\n",
    "\n",
    "            # Perform gradient descent step\n",
    "            optimizer.apply_gradients(zip(grad_theta, self.model.trainable_variables))\n",
    "            return loss, Tloss, ploss, kloss, PI1loss_AD, PI2loss_AD, TpbDloss, TbNloss\n",
    "\n",
    "        #@tf.function\n",
    "        def valds_step():\n",
    "            loss,Tloss,ploss,kloss,PI1loss_AD,PI2loss_AD,TpbDloss,TbNloss,grad_theta \\\n",
    "                = self.get_grad(X,\n",
    "                                Y_pbN, Y_TbN,\n",
    "                                X_wells, T_wells, p_wells, k_wells,\n",
    "                                well_neighbor_indices, dstype)\n",
    "            return loss, Tloss, ploss, kloss, PI1loss_AD, PI2loss_AD, TpbDloss, TbNloss\n",
    "\n",
    "        #@tf.function\n",
    "        def allds_step():\n",
    "            loss,Tloss,ploss,kloss,PI1loss_AD,PI2loss_AD,TpbDloss,TbNloss,grad_theta \\\n",
    "                = self.get_grad(X,\n",
    "                                Y_pbN, Y_TbN,\n",
    "                                X_wells, T_wells, p_wells, k_wells,\n",
    "                                well_neighbor_indices, dstype)\n",
    "            return loss, Tloss, ploss, kloss, PI1loss_AD, PI2loss_AD, TpbDloss, TbNloss\n",
    "\n",
    "        for epoch in range(N):\n",
    "            for (X, Y_pbN,Y_TbN,\\\n",
    "                X_wells, T_wells, p_wells, k_wells,\n",
    "                well_neighbor_indices) in train_ds.shuffle(1000).batch(25).prefetch(1):\n",
    "                dstype = DataSet.TRAIN # Dataset type: training\n",
    "                loss,Tloss,ploss,kloss,PI1loss_AD,PI2loss_AD,TpbDloss,TbNloss = train_step()\n",
    "                self.current_loss = loss.numpy()\n",
    "                self.Tloss = Tloss.numpy()\n",
    "                self.ploss = ploss.numpy()\n",
    "                self.kloss = kloss.numpy()\n",
    "                self.PI1loss_AD = PI1loss_AD.numpy()\n",
    "                self.PI2loss_AD = PI2loss_AD.numpy()\n",
    "                self.TpbDloss = TpbDloss.numpy()\n",
    "                self.TbNloss = TbNloss.numpy()\n",
    "\n",
    "            for (X, Y_pbN,Y_TbN,\\\n",
    "                X_wells, T_wells, p_wells, k_wells,\n",
    "                well_neighbor_indices) in val_ds.shuffle(1000).batch(50).prefetch(1):\n",
    "                dstype = DataSet.VAL # Dataset type: validation\n",
    "                valloss,Tloss,ploss,kloss,PI1loss_AD,PI2loss_AD,TpbDloss,TbNloss = valds_step()\n",
    "                self.val_loss = valloss.numpy()\n",
    "\n",
    "            for (X, Y_pbN,Y_TbN,\\\n",
    "                X_wells, T_wells, p_wells, k_wells,\n",
    "                well_neighbor_indices) in all_ds.shuffle(1000).batch(50).prefetch(1):\n",
    "                dstype = DataSet.ALL # Dataset type: all\n",
    "                allloss,Tloss,ploss,kloss,PI1loss_AD_all,PI2loss_AD_all,TpbDloss,TbNloss = allds_step()\n",
    "                self.PI1all_AD_loss = PI1loss_AD_all.numpy()\n",
    "                self.PI2all_AD_loss = PI2loss_AD_all.numpy()\n",
    "\n",
    "            self.callback()\n",
    "\n",
    "    def solve_with_tfp_lbfgs(self, train_ds, val_ds, all_ds, max_iterations, debug_print=False):\n",
    "\n",
    "        self.max_iteration = max_iterations\n",
    "\n",
    "        def get_loss_for_dataset(dataset:tf.data.Dataset, dstype: DataSet):\n",
    "            \"\"\"\n",
    "            Get loss and loss components:\n",
    "            (loss, Tloss, ploss, kloss, PI1loss, PI2loss, TpbDloss, pbNloss, TbNloss)\n",
    "            \"\"\"\n",
    "            data, = tuple(dataset.batch(1).as_numpy_iterator())\n",
    "            \n",
    "            (X, Y_pbN, Y_TbN, X_wells, T_wells, p_wells, k_wells,\n",
    "             well_neighbor_indices) = data\n",
    "            *losses, _ = self.get_grad(tf.convert_to_tensor(X),\n",
    "                                       Y_pbN, Y_TbN,\n",
    "                                       X_wells, T_wells, p_wells, k_wells,\n",
    "                                       well_neighbor_indices, dstype)\n",
    "            return losses\n",
    "\n",
    "        def loss_gradient_function(X,\n",
    "                                   Y_pbN, Y_TbN,\n",
    "                                   X_wells, T_wells, p_wells, k_wells,\n",
    "                                   well_neighbor_indices, dstype):\n",
    "            \"\"\"A funtion that returns loss and gradient for L-BFGS\"\"\"\n",
    "            losses = self.get_grad(X,\n",
    "                                   Y_pbN, Y_TbN,\n",
    "                                   X_wells, T_wells, p_wells, k_wells,\n",
    "                                   well_neighbor_indices, dstype)\n",
    "\n",
    "            (loss, Tloss, ploss, kloss, PI1loss_AD, PI2loss_AD,\n",
    "             TpbDloss, TbNloss, gradient) = losses\n",
    "\n",
    "            if debug_print:\n",
    "                print(f'[L-BFGS] objective evaluated: loss value = {float(loss)}')\n",
    "            return loss, gradient\n",
    "\n",
    "        # Get training data\n",
    "        train_data, = tuple(train_ds.batch(1).as_numpy_iterator())\n",
    "        (X, Y_pbN, Y_TbN, X_wells, T_wells, p_wells, k_wells,\n",
    "         well_neighbor_indices) = train_data\n",
    "\n",
    "        # Initialize L-BFGS optimizer\n",
    "        lbfgs_optimizer = LBfgsOptimizer(\n",
    "            model=self.model,\n",
    "            loss_function=loss_gradient_function,\n",
    "            loss_function_kwargs=dict(\n",
    "                X=tf.convert_to_tensor(X),\n",
    "                Y_pbN=Y_pbN, Y_TbN=Y_TbN,\n",
    "                X_wells=X_wells, T_wells=T_wells, p_wells=p_wells, k_wells=k_wells,\n",
    "                well_neighbor_indices=well_neighbor_indices, dstype=DataSet.TRAIN),\n",
    "            lbfgs_kwargs=dict(  # Set the arguments to be passed to tfp.optimize.lbfgs_minimize()\n",
    "                tolerance=1e-12,\n",
    "                max_line_search_iterations=100,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Execute L-BFGS optimizer\n",
    "        current_iterations = 0\n",
    "        result_state = None\n",
    "        callback_interval = 1\n",
    "        while current_iterations < max_iterations:\n",
    "\n",
    "            # Execute a single interation of L-BFGS optimization\n",
    "            result_state = lbfgs_optimizer.optimize(\n",
    "                max_iterations=min(current_iterations+callback_interval, max_iterations)\n",
    "            )\n",
    "\n",
    "            # Update the number of epoch\n",
    "            current_iterations = result_state.num_total_iterations\n",
    "            if debug_print:\n",
    "                print(f'[L-BFGS] current result state: {result_state}')\n",
    "\n",
    "            # Calculate loss components using the temporaily trained PINN\n",
    "            loss, Tloss, ploss, kloss, PI1loss_AD, PI2loss_AD, TpbDloss, TbNloss = get_loss_for_dataset(train_ds, DataSet.TRAIN)\n",
    "            self.current_loss = loss.numpy()\n",
    "            self.Tloss = Tloss.numpy()\n",
    "            self.ploss = ploss.numpy()\n",
    "            self.kloss = kloss.numpy()\n",
    "            self.PI1loss_AD = PI1loss_AD.numpy()\n",
    "            self.PI2loss_AD = PI2loss_AD.numpy()\n",
    "            self.TpbDloss = TpbDloss.numpy()\n",
    "            self.TbNloss = TbNloss.numpy()\n",
    "\n",
    "            val_loss, *_ = get_loss_for_dataset(val_ds, DataSet.VAL)\n",
    "            self.val_loss = val_loss.numpy()\n",
    "\n",
    "            all_loss, _, _, _, PI1loss_all_AD, PI2loss_all_AD, _, _ = get_loss_for_dataset(all_ds, DataSet.ALL)\n",
    "            self.PI1all_AD_loss = PI1loss_all_AD.numpy()\n",
    "            self.PI2all_AD_loss = PI2loss_all_AD.numpy()\n",
    "\n",
    "            # Finish iteration once L-BFGS is converged\n",
    "            if result_state.converged:\n",
    "                print(f'[L-BFGS] converged: state={result_state}.')\n",
    "                # callback\n",
    "                self.callback(force_all_output=True)\n",
    "                return result_state\n",
    "\n",
    "            # Finish the iteration when L-BFGS is failed to converge\n",
    "            # L-BFGS of TensorFlow Probablity failed under the following condition:\n",
    "            # \"a line search step failed to find a suitable step size satisfying Wolfe conditions\"\n",
    "            # (https://www.tensorflow.org/probability/api_docs/python/tfp/optimizer/lbfgs_minimize)\n",
    "            if result_state.failed:\n",
    "                print(f'[L-BFGS] line search step failed: state={result_state}.')\n",
    "                # Execute callback\n",
    "                self.callback(force_all_output=True)\n",
    "                return result_state\n",
    "\n",
    "            # callback\n",
    "            self.callback()\n",
    "\n",
    "        print(f'[L-BFGS] reached max iteration: state={result_state}.')\n",
    "        return result_state\n",
    "\n",
    "    def retrieve_all_point_indices(self, model_input_grid):\n",
    "        \n",
    "        x_size = model_input_grid.shape[1]\n",
    "        y_size = model_input_grid.shape[2]\n",
    "        z_size = model_input_grid.shape[3]\n",
    "        all_point_indices_np = np.zeros(shape=(x_size * y_size * z_size, 3), dtype=int)\n",
    "\n",
    "        for iz in range(z_size):\n",
    "            for iy in range(y_size):\n",
    "                for ix in range(x_size):\n",
    "                    all_point_indices_np[ix + iy * x_size + iz * x_size * y_size, 0] = ix\n",
    "                    all_point_indices_np[ix + iy * x_size + iz * x_size * y_size, 1] = iy\n",
    "                    all_point_indices_np[ix + iy * x_size + iz * x_size * y_size, 2] = iz\n",
    "\n",
    "        all_point_indices = tf.constant(all_point_indices_np)\n",
    "\n",
    "        return all_point_indices\n",
    "\n",
    "    def flatten_list(self,nested_list):\n",
    "        flat_list = []\n",
    "        for item in nested_list:\n",
    "            if isinstance(item, list):\n",
    "                flat_list.extend(self.flatten_list(item))\n",
    "            else:\n",
    "                flat_list.append(item)\n",
    "        return flat_list\n",
    "    \n",
    "    def callback(self, xr=None, force_all_output=False):\n",
    "\n",
    "        if self.iter % 100 == 0:\n",
    "            print('#----------------------#')\n",
    "            print('It {:05d}:'.format(self.iter))\n",
    "            print('trainloss,valloss = {:10.8e},{:10.8e}'.format(self.current_loss,self.val_loss))\n",
    "            print('Tloss,Ploss,Kloss {:10.8e},{:10.8e},{:10.8e}'.format(self.Tloss,self.ploss,self.kloss))\n",
    "            print('Mass_loss,Energy_loss {:10.8e},{:10.8e}'.format(self.PI1loss_AD,self.PI2loss_AD))\n",
    "            print('TpbDloss,TpNloss {:10.8e},{:10.8e}'.format(self.TpbDloss,self.TbNloss))\n",
    "\n",
    "        if self.iter % 5000 == 0:\n",
    "            checkpoint = tf.train.Checkpoint(model=self.model.variables)\n",
    "            manager = tf.train.CheckpointManager(checkpoint, directory='./save_checkpoints', checkpoint_name='weights.ckpt', max_to_keep=500)\n",
    "            path = manager.save(checkpoint_number=self.iter)\n",
    "            print(\"weights saved to %s\" % path)\n",
    "\n",
    "        self.train_hist.append(self.current_loss)\n",
    "        self.val_hist.append(self.val_loss)\n",
    "        self.Tloss_hist.append(self.Tloss)\n",
    "        self.ploss_hist.append(self.ploss)\n",
    "        self.kloss_hist.append(self.kloss)\n",
    "        self.PI1lossAD_hist.append(self.PI1loss_AD)\n",
    "        self.PI2lossAD_hist.append(self.PI2loss_AD)\n",
    "        self.TpbDloss_hist.append(self.TpbDloss)\n",
    "        self.TbNloss_hist.append(self.TbNloss)\n",
    "        self.PI1lossAD_all_hist.append(self.PI1all_AD_loss)\n",
    "        self.PI2lossAD_all_hist.append(self.PI2all_AD_loss)\n",
    "\n",
    "        self.gTPK = []\n",
    "        self.gPI1 = []\n",
    "        self.gPI2 = []\n",
    "        self.iter+=1\n",
    "\n",
    "        if self.iter % 100 == 0 or self.iter == 1 or force_all_output:\n",
    "            if not os.path.exists('./save_predict'):\n",
    "                os.makedirs('./save_predict')\n",
    "                \n",
    "            T_grid, p_grid, k_grid = self.model(self._model_input_grid)\n",
    "\n",
    "            Tdenm = self.reshape_and_rescale(T_grid, var='T')\n",
    "            pdenm = self.reshape_and_rescale(p_grid, var='p')\n",
    "            kdenm = self.reshape_and_rescale(k_grid, var='k')\n",
    "\n",
    "            all_point_indices = self.retrieve_all_point_indices(self._model_input_grid)\n",
    "            grid_shape = T_grid.shape\n",
    "            Tdenm = self.extract_points(\n",
    "                in_tensor=Tdenm, grid_shape=grid_shape, var=VariableType.T,\n",
    "                model_input_grid=self._model_input_grid,\n",
    "                point_indices=all_point_indices)\n",
    "            pdenm = self.extract_points(\n",
    "                in_tensor=pdenm, grid_shape=grid_shape, var=VariableType.p,\n",
    "                model_input_grid=self._model_input_grid,\n",
    "                point_indices=all_point_indices)\n",
    "\n",
    "            Tdenm_df = pd.DataFrame(Tdenm.numpy())\n",
    "            pdenm_df = pd.DataFrame(pdenm.numpy())\n",
    "            kdenm_df = pd.DataFrame(kdenm.numpy())\n",
    "            Tpkdenm_df = pd.concat([Tdenm_df,pdenm_df,kdenm_df],axis=1)\n",
    "            Tpkdenm_df.columns=['T_pred','p_pred','k_pred']\n",
    "            Tpkdenm_df.to_csv('./save_predict/predicted_'+str(self.iter)+'.csv',index=False)\n",
    "            # Calculate L2 error\n",
    "            N = self.xall.shape[0]\n",
    "            t = Tdenm.numpy().reshape(N)\n",
    "            t_true = self.T_true.reshape(N)\n",
    "            p = pdenm.numpy().reshape(N)\n",
    "            p_true = self.p_true.reshape(N)\n",
    "            k = kdenm.numpy().reshape(N)\n",
    "            k_true = self.k_true.reshape(N)\n",
    "\n",
    "            err_T = np.sqrt(np.dot(t - t_true, t - t_true)/np.dot(t_true, t_true))\n",
    "            err_p = np.sqrt(np.dot(p - p_true, p - p_true)/np.dot(p_true, p_true))\n",
    "            err_k = np.sqrt(np.dot(k - k_true, k - k_true)/np.dot(k_true, k_true))\n",
    "            print(self.iter, err_T, err_p, err_k)\n",
    "            filename = \"./save_checkpoints/error_\" + \"lmode_\" + str(self.lmode) + \".csv\"\n",
    "            header = ''\n",
    "            if (self.iter == 1):\n",
    "                header = 'Iterations, L2-error(T), L2-error (P), L2-error (log k)'\n",
    "\n",
    "            with open(filename, 'a') as f:\n",
    "                errorlist = np.array([self.iter, err_T, err_p, err_k]).reshape(1,4)\n",
    "                np.savetxt(f, errorlist, delimiter = ',', header = header, comments = '')\n",
    "                \n",
    "        if self.iter % 1000 == 0 or force_all_output:\n",
    "            train_nd = pd.DataFrame([self.train_hist])\n",
    "            val_nd = pd.DataFrame([self.val_hist])\n",
    "            Tnet_nd = pd.DataFrame([self.Tloss_hist])\n",
    "            Pnet_nd = pd.DataFrame([self.ploss_hist])\n",
    "            Knet_nd = pd.DataFrame([self.kloss_hist])\n",
    "            PI1_AD_nd = pd.DataFrame([self.PI1lossAD_hist])\n",
    "            PI2_AD_nd = pd.DataFrame([self.PI2lossAD_hist])\n",
    "            TpbD_nd = pd.DataFrame([self.TpbDloss_hist])\n",
    "            TbN_nd = pd.DataFrame([self.TbNloss_hist])\n",
    "            PI1_AD_all_nd = pd.DataFrame([self.PI1lossAD_all_hist])\n",
    "            PI2_AD_all_nd = pd.DataFrame([self.PI2lossAD_all_hist])\n",
    "            wPI1_nd = pd.DataFrame([self.wPI1_hist])\n",
    "            wPI2_nd = pd.DataFrame([self.wPI2_hist])\n",
    "            wTbN_nd = pd.DataFrame([self.wTbN_hist])\n",
    "            histories_df = pd.concat([train_nd.T,val_nd.T,Tnet_nd.T,Pnet_nd.T,Knet_nd.T,PI1_AD_nd.T,PI2_AD_nd.T,TpbD_nd.T,TbN_nd.T,PI1_AD_all_nd.T,PI2_AD_all_nd.T,wPI1_nd.T,wPI2_nd.T,wTbN_nd.T],axis=1)\n",
    "            histories_df.columns = ['train_loss','val_loss','Tnet_loss','Pnet_loss','Knet_loss','PI1_loss_AD','PI2_loss_AD','TpbDnet_loss','TbN_loss','PI1_allgrid','PI2_allgrid','Weight_PI1loss','Weight_PI2loss','Weight_TbNloss']\n",
    "            histories_df.to_csv('./save_checkpoints/histories_loss.csv',index=False)\n",
    "\n",
    "            # plot\n",
    "            N = self.xall.shape[0]\n",
    "            x1 = tf.reshape(self.xall[:, 0], (N, 1))\n",
    "            x2 = tf.reshape(self.xall[:, 1], (N, 1))\n",
    "            x3 = tf.reshape(self.xall[:, 2], (N, 1))\n",
    "\n",
    "            N = self.xall.shape[0]\n",
    "            t = Tdenm.numpy().reshape(N)\n",
    "            t_true = self.T_true.reshape(N)\n",
    "            p = pdenm.numpy().reshape(N)\n",
    "            p_true = self.p_true.reshape(N)\n",
    "            k = kdenm.numpy().reshape(N)\n",
    "            k_true = self.k_true.reshape(N)\n",
    "\n",
    "            err_p = np.abs(p - p_true)/p_true\n",
    "\n",
    "            filename = \"./save_checkpoints/T_\" + str(self.iter) + \"lmode_\" + str(self.lmode) + \".png\"\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(projection='3d')\n",
    "            cm = plt.cm.get_cmap('hot')\n",
    "            pl = ax.scatter(self.xall[:, 0], self.xall[:, 1],self.xall[:, 2], vmin = 15, vmax = 350,c = Tdenm, cmap=cm)\n",
    "            plt.xlabel(\"x\")\n",
    "            plt.ylabel(\"z\")\n",
    "            plt.title(\"T\")\n",
    "            plt.colorbar(pl)\n",
    "            plt.savefig(filename)\n",
    "            plt.clf()\n",
    "\n",
    "            filename = \"./save_checkpoints/p_\" + str(self.iter) + \"lmode_\" + str(self.lmode) + \".png\"\n",
    "            cm = plt.cm.get_cmap('cool')\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(projection='3d')\n",
    "            pl = ax.scatter(self.xall[:, 0], self.xall[:, 1], self.xall[:, 2],c = pdenm,vmin = 1e5, vmax = 2.5e7, cmap=cm)\n",
    "            plt.xlabel(\"x\")\n",
    "            plt.ylabel(\"z\")\n",
    "            plt.title(\"p\")\n",
    "            plt.colorbar(pl)\n",
    "            plt.savefig(filename)\n",
    "            plt.clf()\n",
    "\n",
    "            filename = \"./save_checkpoints/k_\" + str(self.iter) + \"lmode_\" + str(self.lmode) + \".png\"\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(projection='3d')\n",
    "            cm = plt.cm.get_cmap('jet')\n",
    "            pl = ax.scatter(self.xall[:, 0], self.xall[:, 1], self.xall[:,2],c = kdenm,vmin = -17, vmax = -12, cmap=cm)\n",
    "            plt.xlabel(\"x\")\n",
    "            plt.ylabel(\"z\")\n",
    "            plt.title(\"k\")\n",
    "\n",
    "            plt.colorbar(pl)\n",
    "            plt.savefig(filename)\n",
    "            plt.clf()\n",
    "\n",
    "            plt.close('all')\n",
    "                \n",
    "    def restore_checkpoint(self):\n",
    "        checkpoint = tf.train.Checkpoint(model=self.model.variables)\n",
    "        checkpoint.restore(tf.train.latest_checkpoint('./save_checkpoints'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE='float32'\n",
    "tf.keras.backend.set_floatx(DTYPE)\n",
    "\n",
    "def Normalize_input(val,idx_pos, val_star):\n",
    "    return (val[idx_pos]-np.min(val_star))/(np.max(val_star)-np.min(val_star))\n",
    "\n",
    "def Normalize_input_all(val, val_star):\n",
    "    return (val - np.min(val_star))/(np.max(val_star)-np.min(val_star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell block to read well and reference data\n",
    "\"\"\"\n",
    "\n",
    "# The number of datapoints in x (x1), y (x2), and z (x3) direction of the reference model\n",
    "N_x1 = 18\n",
    "N_x2 = 11\n",
    "N_x3 = 18\n",
    "\n",
    "# The total number of datapoints\n",
    "N = N_x1 * N_x2 * N_x3\n",
    "\n",
    "# Read reference data\n",
    "dataset = pd.read_csv('./Reference_model.csv')\n",
    "X = np.array((dataset['X_Easting'],dataset['Y_Northing'], dataset['Elevation'])).T\n",
    "print(\"shape of X = \", X.shape)\n",
    "T_true = np.array(dataset['T_degC']).reshape(X.shape[0] ,1)\n",
    "k_true = np.array(dataset['log10PER']).reshape(X.shape[0] ,1)\n",
    "p_true = np.array(dataset['P_Pa']).reshape(X.shape[0] ,1)\n",
    "\n",
    "# Read well coordinates & temperatures, pore pressures, permeabilities at wells\n",
    "dataset_wells = pd.read_csv('./Welldata_30wells.csv')\n",
    "X_star = np.array((dataset_wells['X_Easting'],dataset_wells['Y_Northing'], dataset_wells['Elevation'])).T\n",
    "T_star = np.array(dataset_wells['T_degC']).reshape(X_star.shape[0] ,1)\n",
    "k_star = np.array(dataset_wells['log10PER']).reshape(X_star.shape[0] ,1)\n",
    "p_star = np.array(dataset_wells['P_Pa']).reshape(X_star.shape[0] ,1)\n",
    "T_star_tf = tf.constant(T_star, DTYPE)\n",
    "p_star_tf = tf.constant(p_star, DTYPE)\n",
    "k_star_tf = tf.constant(k_star, DTYPE)\n",
    "\n",
    "# Elevation threshold to define the boundary of training and validation data\n",
    "training_validation_boundary = -1700\n",
    "\n",
    "# 領域の境界のインデックスを取得します。\n",
    "idx_low, idx_up, idx_west, idx_east, idx_south, idx_north \\\n",
    "            = gb.get_boundary_indexlist(N_x1, N_x2, N_x3, X)\n",
    "print('idx_low.shape = ', len(idx_low))\n",
    "print('idx_up.shape = ', len(idx_up))\n",
    "print('idx_west.shape = ', len(idx_west))\n",
    "print('idx_east.shape = ', len(idx_east))\n",
    "print('idx_south.shape = ', len(idx_south))\n",
    "print('idx_north.shape = ', len(idx_north))\n",
    "\n",
    "ndx_ew = len(idx_east)\n",
    "ndx_ns = len(idx_south)\n",
    "\n",
    "# Type of the bottom boundary condition\n",
    "if_low_dirichlet = True  # Dirichlet boundary (Temperature & Pressure boundary)\n",
    "#if_low_dirichlet = False  # Neumann boundary (Heat flow boundary)\n",
    "\n",
    "# Heat flow at the bottom boundary\n",
    "qN = 0.6\n",
    "\n",
    "# Representative thermal conductivity\n",
    "Lambda = 2.0\n",
    "\n",
    "# Thermal gradient at the bottom\n",
    "dTdn = qN / Lambda\n",
    "\n",
    "# Number of wells used for interpolation\n",
    "pointnumber_of_interpolate = 1\n",
    "\n",
    "# Interpolation parameters used to create approximate functions for Dirichlet boundary conditions on the top and bottom surfaces\n",
    "order_interpolate_dirichlet = 1\n",
    "regularization_interpolate_dirichlet = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input grid\n",
    "up_indices = (\n",
    "    np.transpose(\n",
    "        np.unravel_index(\n",
    "            idx_up,\n",
    "            shape=(N_x3, N_x2, N_x1)))[:, [2, 1, 0]]\n",
    ")\n",
    "low_indices = (\n",
    "    np.transpose(\n",
    "        np.unravel_index(\n",
    "            idx_low,\n",
    "            shape=(N_x3, N_x2, N_x1)))[:, [2, 1, 0]]\n",
    ")\n",
    "side_indices = (\n",
    "    np.transpose(\n",
    "        np.unravel_index(\n",
    "            np.concatenate([idx_west, idx_east, idx_south, idx_north]),\n",
    "            shape=(N_x3, N_x2, N_x1)))[:, [2, 1, 0]]\n",
    ")\n",
    "grid_spec = GridSpec(\n",
    "    N_x1=N_x1, N_x2=N_x2, N_x3=N_x3,\n",
    "    up_indices=up_indices,\n",
    "    low_indices=low_indices,\n",
    "    side_indices=side_indices\n",
    ")\n",
    "\n",
    "print(grid_spec.up_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell block to specify boundary conditions\n",
    "\"\"\"\n",
    "\n",
    "# Specify input domain bounds\n",
    "lb, ub = X.min(0), X.max(0)\n",
    "# Lower bounds\n",
    "lb = tf.constant(X.min(0), dtype=DTYPE)\n",
    "# Upper bounds\n",
    "ub = tf.constant(X.max(0), dtype=DTYPE)\n",
    "#\n",
    "idx_f = np.random.choice(N,N, replace=False)\n",
    "#------------------------------------------------------------------\n",
    "\n",
    "# Dirichlet condition at the upper surface boundary\n",
    "X_uTpbD1 = tf.constant(X[idx_up, :], dtype=DTYPE)\n",
    "X_uTpbD = [X_uTpbD1,X_uTpbD1]\n",
    "Y_uTpbD_T = tf.constant(Normalize_input(T_true, idx_up, T_star), dtype=DTYPE)\n",
    "Y_uTpbD_p = tf.constant(Normalize_input(p_true, idx_up, p_star), dtype=DTYPE)\n",
    "\n",
    "# Dirichlet condition at the bottom boundary\n",
    "X_lTpbD1 = tf.constant(X[idx_low, :], dtype=DTYPE)\n",
    "X_lTpbD = [X_lTpbD1,X_lTpbD1]\n",
    "Y_lTpbD_T = tf.constant(Normalize_input(T_true, idx_low, T_star), dtype=DTYPE)\n",
    "Y_lTpbD_p = tf.constant(Normalize_input(p_true, idx_low, p_star), dtype=DTYPE)\n",
    "\n",
    "# Neumann condition at the side boundary\n",
    "x_pbN1 = tf.constant(X[idx_west, :], dtype=DTYPE)\n",
    "x_pbN2 = tf.constant(X[idx_east, :], dtype=DTYPE)\n",
    "x_pbN3 = tf.constant(X[idx_south, :], dtype=DTYPE)\n",
    "x_pbN4 = tf.constant(X[idx_north, :], dtype=DTYPE)\n",
    "X_pbN = tf.concat([x_pbN1, x_pbN2, x_pbN3, x_pbN4], axis = 0)\n",
    "Y_pbN = tf.zeros((X_pbN.shape[0], 1), dtype=DTYPE)\n",
    "n_west_vec = tf.constant([[-1.0, 0.0, 0.0]], dtype=DTYPE)\n",
    "n_east_vec = tf.constant([[1.0, 0.0, 0.0]], dtype=DTYPE)\n",
    "n_south_vec = tf.constant([[0.0, -1.0, 0.0]], dtype=DTYPE)\n",
    "n_north_vec = tf.constant([[0.0, 1.0, 0.0]], dtype=DTYPE)\n",
    "n_west_size = tf.constant([N_x2 * N_x3,1], tf.int32)\n",
    "n_east_size = tf.constant([N_x2 * N_x3,1], tf.int32)\n",
    "n_south_size = tf.constant([N_x1 * N_x3,1], tf.int32)\n",
    "n_north_size = tf.constant([N_x1 * N_x3,1], tf.int32)\n",
    "n_west = tf.tile(n_west_vec,n_west_size)\n",
    "n_east = tf.tile(n_east_vec,n_east_size)\n",
    "n_south = tf.tile(n_south_vec,n_south_size)\n",
    "n_north = tf.tile(n_north_vec,n_north_size)\n",
    "nvec_p = tf.concat([n_west, n_east, n_south, n_north], axis = 0)\n",
    "\n",
    "# Neumann condition at the bottom boundary\n",
    "X_TbN = tf.constant(X[idx_low, :], dtype=DTYPE)\n",
    "Y_TbN = dTdn * tf.ones((X_TbN.shape[0], 1), dtype=DTYPE)\n",
    "nTvec = tf.constant([[0.0, 0.0, -1.0]], dtype=DTYPE)\n",
    "nTsize = tf.constant([N_x1 * N_x2, 1], tf.int32)\n",
    "nvec_T = np.tile(nTvec, nTsize)\n",
    "\n",
    "# Coordinates in all analyzed domain\n",
    "x1_all = tf.constant(X[:,0:1], dtype=DTYPE)\n",
    "x2_all = tf.constant(X[:,1:2], dtype=DTYPE)\n",
    "x3_all = tf.constant(X[:,2:3], dtype=DTYPE)\n",
    "X_all = tf.concat([x1_all, x2_all, x3_all], axis=1)\n",
    "#------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell block that obtains grid points near the wells.\n",
    "\"\"\"\n",
    "\n",
    "# Get indices for datapoints near wells\n",
    "idx_tpk_train, idx_tpk_val = swtv.get_wells_neighbor_indecies(\n",
    "                                                             X_star, X,\n",
    "                                                             training_validation_boundary,\n",
    "                                                             pointnumber_of_interpolate)\n",
    "\n",
    "# Divide well data into training and validation data\n",
    "[X_wells_train, T_wells_train_denm, p_wells_train_denm, k_wells_train_denm,\n",
    "X_wells_val, T_wells_val_denm, p_wells_val_denm, k_wells_val_denm]\\\n",
    "= swtv.get_training_and_validation_data(X_star, T_star, p_star,\n",
    "                                        k_star, training_validation_boundary)\n",
    "\n",
    "# Finish in case of no training points\n",
    "if idx_tpk_train.shape[0] == 0:\n",
    "    print(\"There are no training points for the loss function. Exit.\")\n",
    "    sys.exit()\n",
    "# Warning in case of no validation point \n",
    "if idx_tpk_val.shape[0] == 0:\n",
    "    print(\"Warning： There is no validation point. The loss function for the validation data will be NaN.\")\n",
    "\n",
    "N_wells = X_star.shape[0]\n",
    "N_wells_train = X_wells_train.shape[0]\n",
    "N_wells_val = X_wells_val.shape[0]\n",
    "\n",
    "# Training and validation data\n",
    "X_wells_train = tf.constant(X_wells_train, dtype = DTYPE)\n",
    "X_wells_val = tf.constant(X_wells_val, dtype = DTYPE)\n",
    "\n",
    "X_train = []\n",
    "T_train = []\n",
    "p_train = []\n",
    "k_train = []\n",
    "\n",
    "X_val = []\n",
    "T_val = []\n",
    "p_val = []\n",
    "k_val = []\n",
    "\n",
    "N_train = idx_tpk_train.shape[0]\n",
    "N_val = idx_tpk_val.shape[0]\n",
    "\n",
    "X_train = tf.constant(X[idx_tpk_train, :], dtype = DTYPE)\n",
    "T_train = tf.constant(Normalize_input(T_true, idx_tpk_train, T_star), dtype = DTYPE)\n",
    "p_train = tf.constant(Normalize_input(p_true, idx_tpk_train, p_star), dtype = DTYPE)\n",
    "k_train = tf.constant(Normalize_input(k_true, idx_tpk_train, k_star), dtype = DTYPE)\n",
    "\n",
    "X_val = tf.constant(X[idx_tpk_val, :], dtype = DTYPE)\n",
    "T_val = tf.constant(Normalize_input(T_true, idx_tpk_val, T_star), dtype = DTYPE)\n",
    "p_val = tf.constant(Normalize_input(p_true, idx_tpk_val, p_star), dtype = DTYPE)\n",
    "k_val = tf.constant(Normalize_input(k_true, idx_tpk_val, k_star), dtype = DTYPE)\n",
    "\n",
    "# Normalize well data\n",
    "T_wells_train = tf.constant(Normalize_input_all(T_wells_train_denm, T_star), dtype = DTYPE)\n",
    "p_wells_train = tf.constant(Normalize_input_all(p_wells_train_denm, p_star), dtype = DTYPE)\n",
    "k_wells_train = tf.constant(Normalize_input_all(k_wells_train_denm, k_star), dtype = DTYPE)\n",
    "\n",
    "T_wells_val = tf.constant(Normalize_input_all(T_wells_val_denm, T_star), dtype = DTYPE)\n",
    "p_wells_val = tf.constant(Normalize_input_all(p_wells_val_denm, p_star), dtype = DTYPE)\n",
    "k_wells_val = tf.constant(Normalize_input_all(k_wells_val_denm, k_star), dtype = DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell block to create tf.data.Dataset for training and validation data\n",
    "\"\"\"\n",
    "#---------------------------------------------------------\n",
    "train_ds = tf.data.Dataset.from_tensors((X_train,\n",
    "                                        Y_pbN,Y_TbN,\n",
    "                                        X_wells_train,\n",
    "                                        T_wells_train,\n",
    "                                        p_wells_train,\n",
    "                                        k_wells_train,\n",
    "                                        idx_tpk_train\n",
    "                                        ))\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensors((X_val,\n",
    "                                        Y_pbN,\n",
    "                                        Y_TbN,\n",
    "                                        X_wells_val,\n",
    "                                        T_wells_val,\n",
    "                                        p_wells_val,\n",
    "                                        k_wells_val,\n",
    "                                        idx_tpk_val\n",
    "                                    ))\n",
    "\n",
    "#--------------------------------- All grid data\n",
    "idx_all = np.arange(0,N,1)\n",
    "x1_T_all = tf.constant(X[idx_all,0:1], dtype=DTYPE)\n",
    "x2_T_all = tf.constant(X[idx_all,1:2], dtype=DTYPE)\n",
    "x3_T_all = tf.constant(X[idx_all,2:3], dtype=DTYPE)\n",
    "\n",
    "X_T_all = tf.concat([x1_T_all, x2_T_all, x3_T_all], axis=1)\n",
    "Y_T_all = tf.constant(Normalize_input(T_true,idx_all, T_star), dtype=DTYPE)\n",
    "\n",
    "x1_p_all = tf.constant(X[idx_all,0:1], dtype=DTYPE)\n",
    "x2_p_all = tf.constant(X[idx_all,1:2], dtype=DTYPE)\n",
    "x3_p_all = tf.constant(X[idx_all,2:3], dtype=DTYPE)\n",
    "X_p_all = tf.concat([x1_p_all, x2_p_all, x3_p_all], axis=1)\n",
    "Y_p_all = tf.constant(Normalize_input(p_true,idx_all, p_star), dtype=DTYPE)\n",
    "\n",
    "x1_k_all = tf.constant(X[idx_tpk_val,0:1], dtype=DTYPE)\n",
    "x2_k_all = tf.constant(X[idx_tpk_val,1:2], dtype=DTYPE)\n",
    "x3_k_all = tf.constant(X[idx_tpk_val,2:3], dtype=DTYPE)\n",
    "X_k_all = tf.concat([x1_k_all, x2_k_all, x3_k_all], axis=1)\n",
    "Y_k_all = tf.constant(Normalize_input(k_true,idx_all, k_star), dtype=DTYPE)\n",
    "#---------------------------------------------------------\n",
    "X_alldata = X_p_all\n",
    "YT_alldata = Y_T_all\n",
    "Yp_alldata = Y_p_all\n",
    "Yk_alldata = Y_k_all\n",
    "\n",
    "# Dummy data are used for all data except X_alldata, as the all data is rarely used.\n",
    "all_ds = tf.data.Dataset.from_tensors((X_alldata,\n",
    "                                        Y_pbN,Y_TbN,\n",
    "                                        X_wells_val,\n",
    "                                        T_wells_val,\n",
    "                                        p_wells_val,\n",
    "                                        k_wells_val,\n",
    "                                        idx_all\n",
    "                                    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========================================================================\n",
    "##### Run PINN solver\n",
    "# Initialize model\n",
    "\n",
    "model = PINN_NeuralNet3(lb,ub)\n",
    "model.build(input_shape=(None, N_x1, N_x2, N_x3, 3))\n",
    "\n",
    "# Initilize PINN solver\n",
    "lmode = -1\n",
    "cof_PI1_AD = 1\n",
    "cof_PI2_AD = 1\n",
    "cof_pbN = 1\n",
    "cof_TbN = 1\n",
    "solver = PINNSolver_3D(\n",
    "    model=model,\n",
    "    model_input_grid_spec=grid_spec,\n",
    "    if_low_dirichlet=if_low_dirichlet,\n",
    "    PI_indices=idx_f,\n",
    "    X_uTpbD=X_uTpbD,X_lTpbD=X_lTpbD,\n",
    "    X_pbN=X_pbN,nvec_p=nvec_p, X_TbN=X_TbN, nvec_T=nvec_T,\n",
    "    X_all=X_all,\n",
    "    Y_uTpbD_T=Y_uTpbD_T, Y_uTpbD_p=Y_uTpbD_p,\n",
    "    Y_lTpbD_T=Y_lTpbD_T, Y_lTpbD_p=Y_lTpbD_p,\n",
    "    cof_PI1AD=cof_PI1_AD, cof_PI2AD=cof_PI2_AD, cof_pbN=cof_pbN, cof_TbN=cof_TbN,\n",
    "    T_true=T_true, p_true=p_true, k_true=k_true,\n",
    "    T_for_normalize=T_star_tf, p_for_normalize=p_star_tf, k_for_normalize=k_star_tf,\n",
    "    lmode=lmode,\n",
    "    dtype=DTYPE)\n",
    "\n",
    "# Decide which optimizer should be used\n",
    "mode = 'TFoptimizer'\n",
    "\n",
    "if mode == 'TFoptimizer':\n",
    "    print('#================= First Adam training without calculate physics and bound terms')\n",
    "    TF_maxstep = 12001\n",
    "    lr = tf.keras.optimizers.schedules.PiecewiseConstantDecay([3000,10000],[5e-3,2.5e-3,5e-4])\n",
    "    optim = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    solver.solve_with_TFoptimizer(optim, train_ds, val_ds, all_ds, N=TF_maxstep)\n",
    "elif mode == 'L-BFGS':\n",
    "    print('#================= L-BFGS training')\n",
    "    max_iter = 5000\n",
    "    lbfgs_result_state = solver.solve_with_tfp_lbfgs(\n",
    "        train_ds=train_ds, val_ds=val_ds, all_ds=all_ds,\n",
    "        max_iterations=max_iter, debug_print=False)\n",
    "    TF_maxstep = lbfgs_result_state.num_total_iterations\n",
    "\n",
    "os.rename(src='./save_checkpoints/histories_loss.csv', dst='./save_checkpoints/histories_loss_run0.csv')\n",
    "\n",
    "##### Run PINN solver 1\n",
    "cof_PI1_AD = 1\n",
    "cof_PI2_AD = 1\n",
    "cof_pbN = 1\n",
    "cof_TbN = 1\n",
    "\n",
    "# Initialize model\n",
    "lmode = 0\n",
    "solver2 = PINNSolver_3D(\n",
    "    model=model,\n",
    "    model_input_grid_spec=grid_spec,\n",
    "    if_low_dirichlet=if_low_dirichlet,\n",
    "    PI_indices=idx_f,\n",
    "    X_uTpbD=X_uTpbD,X_lTpbD=X_lTpbD,\n",
    "    X_pbN=X_pbN,nvec_p=nvec_p, X_TbN=X_TbN, nvec_T=nvec_T,\n",
    "    X_all=X_all,\n",
    "    Y_uTpbD_T=Y_uTpbD_T, Y_uTpbD_p=Y_uTpbD_p,\n",
    "    Y_lTpbD_T=Y_lTpbD_T, Y_lTpbD_p=Y_lTpbD_p,\n",
    "    cof_PI1AD=cof_PI1_AD, cof_PI2AD=cof_PI2_AD, cof_pbN=cof_pbN, cof_TbN=cof_TbN,\n",
    "    T_true=T_true, p_true=p_true, k_true=k_true,\n",
    "    T_for_normalize=T_star_tf, p_for_normalize=p_star_tf, k_for_normalize=k_star_tf,\n",
    "    lmode=lmode,\n",
    "    dtype=DTYPE)\n",
    "\n",
    "solver2.restore_checkpoint\n",
    "\n",
    "mode = 'TFoptimizer'\n",
    "\n",
    "if mode == 'TFoptimizer':\n",
    "    print('#================= First Adam training')\n",
    "    TF_maxstep = 2001\n",
    "    lr = tf.keras.optimizers.schedules.PiecewiseConstantDecay([3000,10000],[5e-4,1e-4,5e-5])\n",
    "    optim = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    solver2.solve_with_TFoptimizer(optim, train_ds, val_ds, all_ds, N=TF_maxstep)\n",
    "elif mode == 'L-BFGS':\n",
    "    print('#================= L-BFGS training')\n",
    "    max_iter = 20000\n",
    "    solver2.solve_with_tfp_lbfgs(\n",
    "        train_ds=train_ds, val_ds=val_ds, all_ds=all_ds,\n",
    "        max_iterations=max_iter, debug_print=True)\n",
    "\n",
    "os.rename(src='./save_checkpoints/histories_loss.csv', dst='./save_checkpoints/histories_loss_run1.csv')\n",
    "\n",
    "##### Run PINN solver 2\n",
    "# read loss history file and calc weighting coefs for PI1loss & PI2loss\n",
    "lhistries_df = pd.read_csv('./save_checkpoints/histories_loss_run1.csv')\n",
    "Tloss = lhistries_df.Tnet_loss\n",
    "Ploss = lhistries_df.Pnet_loss\n",
    "Kloss = lhistries_df.Knet_loss\n",
    "TbNloss = lhistries_df.TbN_loss\n",
    "Tloss_last = Tloss[TF_maxstep-2]\n",
    "Ploss_last = Ploss[TF_maxstep-2]\n",
    "Kloss_last = Kloss[TF_maxstep-2]\n",
    "TbNloss_last = TbNloss[TF_maxstep-2]\n",
    "PI1loss_AD = lhistries_df.PI1_loss_AD\n",
    "PI2loss_AD = lhistries_df.PI2_loss_AD\n",
    "PI1loss_AD_last = PI1loss_AD[TF_maxstep-2]\n",
    "PI2loss_AD_last = PI2loss_AD[TF_maxstep-2]\n",
    "aveloss_last = (Tloss_last+Ploss_last+Kloss_last)/3\n",
    "cof_PI1_AD = aveloss_last / PI1loss_AD_last\n",
    "cof_PI2_AD = aveloss_last / PI2loss_AD_last\n",
    "\n",
    "cof_pbN = 0.0\n",
    "if TbNloss_last == 0.0:\n",
    "    cof_TbN = 0.0\n",
    "else:\n",
    "    cof_TbN = aveloss_last / TbNloss_last\n",
    "\n",
    "# Initialize model\n",
    "lmode = 1\n",
    "solver2 = PINNSolver_3D(\n",
    "    model=model,\n",
    "    model_input_grid_spec=grid_spec,\n",
    "    if_low_dirichlet=if_low_dirichlet,\n",
    "    PI_indices=idx_f,\n",
    "    X_uTpbD=X_uTpbD,X_lTpbD=X_lTpbD,\n",
    "    X_pbN=X_pbN,nvec_p=nvec_p, X_TbN=X_TbN, nvec_T=nvec_T,\n",
    "    X_all=X_all,\n",
    "    Y_uTpbD_T=Y_uTpbD_T, Y_uTpbD_p=Y_uTpbD_p,\n",
    "    Y_lTpbD_T=Y_lTpbD_T, Y_lTpbD_p=Y_lTpbD_p,\n",
    "    cof_PI1AD=cof_PI1_AD, cof_PI2AD=cof_PI2_AD, cof_pbN=cof_pbN, cof_TbN=cof_TbN,\n",
    "    T_true=T_true, p_true=p_true, k_true=k_true,\n",
    "    T_for_normalize=T_star_tf, p_for_normalize=p_star_tf, k_for_normalize=k_star_tf,\n",
    "    lmode=lmode,\n",
    "    dtype=DTYPE)\n",
    "\n",
    "solver2.restore_checkpoint\n",
    "\n",
    "mode = 'TFoptimizer'\n",
    "\n",
    "if mode == 'TFoptimizer':\n",
    "    print('#================= Second Adam training')\n",
    "    TF_maxstep = 30001\n",
    "    lr = tf.keras.optimizers.schedules.PiecewiseConstantDecay([3000,10000],[5e-4,1e-4,5e-5])\n",
    "    optim = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    solver2.solve_with_TFoptimizer(optim, train_ds, val_ds, all_ds, N=TF_maxstep)\n",
    "elif mode == 'L-BFGS':\n",
    "    print('#================= L-BFGS training')\n",
    "    max_iter = 2000\n",
    "    solver2.solve_with_tfp_lbfgs(\n",
    "        train_ds=train_ds, val_ds=val_ds, all_ds=all_ds,\n",
    "        max_iterations=max_iter, debug_print=True)\n",
    "\n",
    "os.rename(src='./save_checkpoints/histories_loss.csv', dst='./save_checkpoints/histories_loss_run2.csv')\n",
    "\n",
    "\n",
    "##### Run PINN solver 3\n",
    "# Read loss history file and calc weighting coefs for PI1loss & PI2loss\n",
    "lhistries_df = pd.read_csv('./save_checkpoints/histories_loss_run2.csv')\n",
    "Tloss = lhistries_df.Tnet_loss\n",
    "Ploss = lhistries_df.Pnet_loss\n",
    "Kloss = lhistries_df.Knet_loss\n",
    "TbNloss = lhistries_df.TbN_loss\n",
    "Tloss_last = Tloss[TF_maxstep-2]\n",
    "Ploss_last = Ploss[TF_maxstep-2]\n",
    "Kloss_last = Kloss[TF_maxstep-2]\n",
    "TbNloss_last = TbNloss[TF_maxstep-2]\n",
    "PI1loss_AD = lhistries_df.PI1_loss_AD\n",
    "PI2loss_AD = lhistries_df.PI2_loss_AD\n",
    "PI1loss_AD_last = PI1loss_AD[TF_maxstep-2]\n",
    "PI2loss_AD_last = PI2loss_AD[TF_maxstep-2]\n",
    "aveloss_last = (Tloss_last+Ploss_last+Kloss_last)/3\n",
    "cof_PI1_AD = aveloss_last / PI1loss_AD_last\n",
    "cof_PI2_AD = aveloss_last / PI2loss_AD_last\n",
    "\n",
    "cof_pbN = 0.0\n",
    "if TbNloss_last == 0.0:\n",
    "    cof_TbN = 0.0\n",
    "else:\n",
    "    cof_TbN = aveloss_last / TbNloss_last\n",
    "\n",
    "# Initialize model\n",
    "lmode = 2\n",
    "solver3 = PINNSolver_3D(\n",
    "    model=model,\n",
    "    model_input_grid_spec=grid_spec,\n",
    "    if_low_dirichlet=if_low_dirichlet,\n",
    "    PI_indices=idx_f,\n",
    "    X_uTpbD=X_uTpbD,X_lTpbD=X_lTpbD,\n",
    "    X_pbN=X_pbN,nvec_p=nvec_p, X_TbN=X_TbN, nvec_T=nvec_T,\n",
    "    X_all=X_all,\n",
    "    Y_uTpbD_T=Y_uTpbD_T, Y_uTpbD_p=Y_uTpbD_p,\n",
    "    Y_lTpbD_T=Y_lTpbD_T, Y_lTpbD_p=Y_lTpbD_p,\n",
    "    cof_PI1AD=cof_PI1_AD, cof_PI2AD=cof_PI2_AD, cof_pbN=cof_pbN, cof_TbN=cof_TbN,\n",
    "    T_true=T_true, p_true=p_true, k_true=k_true,\n",
    "    T_for_normalize=T_star_tf, p_for_normalize=p_star_tf, k_for_normalize=k_star_tf,\n",
    "    lmode=lmode,\n",
    "    dtype=DTYPE)\n",
    "\n",
    "solver3.restore_checkpoint\n",
    "\n",
    "mode = 'L-BFGS'\n",
    "\n",
    "if mode == 'TFoptimizer':\n",
    "    print('#================= Third Adam training')\n",
    "    lr = tf.keras.optimizers.schedules.PiecewiseConstantDecay([3000,10000],[5e-4,1e-4,5e-5])\n",
    "    optim = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    solver3.solve_with_TFoptimizer(optim, train_ds, val_ds, all_ds, N=10001)\n",
    "elif mode == 'L-BFGS':\n",
    "    print('#================= L-BFGS training')\n",
    "    max_iter = 10000\n",
    "    solver3.solve_with_tfp_lbfgs(\n",
    "        train_ds=train_ds, val_ds=val_ds, all_ds=all_ds,\n",
    "        max_iterations=max_iter, debug_print=True)\n",
    "\n",
    "os.rename(src='./save_checkpoints/histories_loss.csv', dst='./save_checkpoints/histories_loss_run3_LBFGS.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
