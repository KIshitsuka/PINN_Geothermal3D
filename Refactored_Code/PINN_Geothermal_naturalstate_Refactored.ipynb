{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics-informed neural network for 3D inverse modeling of natural-state geothermal systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defines prameters for the PINN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from enum import Enum, auto\n",
    "\n",
    "\n",
    "class OptimizationMode(Enum):\n",
    "    \"\"\"\n",
    "    Enumeration type that represents the optimization mode.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    TF\n",
    "    \"\"\"\n",
    "    TF = auto()\n",
    "    LBFGS = auto()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TaskConfiguration:\n",
    "    \"\"\"\n",
    "    Data class that holds the settings information for a task.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    dtype: str\n",
    "        Data type\n",
    "    data_csv: str\n",
    "        Path of the CSV file\n",
    "    x1_column_name: str\n",
    "        Column name for x axis in the CSV file\n",
    "    x2_column_name: str\n",
    "        Column name for y axis\n",
    "    x3_column_name: str\n",
    "        Column name for z axis (elevation)\n",
    "    T_column_name: str\n",
    "        Column name for temperature\n",
    "    p_column_name: str\n",
    "        Column name for pressure\n",
    "    k_column_name: str\n",
    "        Column name for logarithm of permeability\n",
    "    N_x1: int\n",
    "        The number of data points along x axis\n",
    "    N_x2: int\n",
    "        The number of data points along y axis\n",
    "    N_x3: int\n",
    "        The number of data points along z axis\n",
    "    data_wells_csv: str\n",
    "        Path of the CSV file for well data\n",
    "    x1_column_name_wells: str\n",
    "        Column name for x-axis in the well data\n",
    "    x2_column_name_wells: str\n",
    "        Column name for y-axis in the well data\n",
    "    x3_column_name_wells: str\n",
    "        Column name for z-axis in the well data\n",
    "    T_column_name_wells: str\n",
    "        Column name for temperature in the well data\n",
    "    p_column_name_wells: str\n",
    "        Column name for pressure in the well data\n",
    "    k_column_name_wells: str\n",
    "        Column name for permeability in the well data\n",
    "    observation_point_min_elevation_for_training: int\n",
    "        Min elevation as a threshold to determine training data\n",
    "    observation_point_max_elevation_for_validation: int\n",
    "        Max elevation as a threshold to determine validation data\n",
    "    number_of_collocation_points: int\n",
    "        The number of collocation points\n",
    "    qN: float\n",
    "        Heat flow at the bottom boundary\n",
    "    Lambda: float\n",
    "        Thermal conductivity\n",
    "    fix_random_numbers: bool\n",
    "        A flag that determine whether random numbers are fixed\n",
    "    seed: int\n",
    "        Seed number to generate random numbers\n",
    "    first_optimization_mode: OptimizationMode\n",
    "        Optimization mode for the first training\n",
    "    second_optimization_mode: OptimizationMode\n",
    "        Optimization mode for the second training\n",
    "    third_optimization_mode: OptimizationMode\n",
    "        Optimization mode for the third training\n",
    "    fourth_optimization_mode: OptimizationMode\n",
    "        Optimization mode for the fourth training\n",
    "    first_optimization_lr: tf.keras.optimizers.schedules.LearningRateSchedule\n",
    "        Learning rate for the first training\n",
    "    second_optimization_lr: tf.keras.optimizers.schedules.LearningRateSchedule\n",
    "        Learning rate for the second training\n",
    "    third_optimization_lr: tf.keras.optimizers.schedules.LearningRateSchedule\n",
    "        Learning rate for the third training\n",
    "    fourth_optimization_lr: tf.keras.optimizers.schedules.LearningRateSchedule\n",
    "        Learning rate for the fourth training\n",
    "    first_optimization_steps: int\n",
    "        Number of maximum iteration (epoch) in the first training\n",
    "    second_optimization_steps: int\n",
    "        Number of maximum iteration (epoch) in the second training\n",
    "    third_optimization_steps: int\n",
    "        Number of maximum iteration (epoch) in the third training\n",
    "    fourth_optimization_steps: int\n",
    "        Number of maximum iteration (epoch) in the fourth training\n",
    "    \"\"\"\n",
    "\n",
    "    dtype: str\n",
    "    data_csv: str\n",
    "    x1_column_name: str\n",
    "    x2_column_name: str\n",
    "    x3_column_name: str\n",
    "    T_column_name: str\n",
    "    p_column_name: str\n",
    "    k_column_name: str\n",
    "    N_x1: int\n",
    "    N_x2: int\n",
    "    N_x3: int\n",
    "    data_wells_csv: str\n",
    "    x1_column_name_wells: str\n",
    "    x2_column_name_wells: str\n",
    "    x3_column_name_wells: str\n",
    "    T_column_name_wells: str\n",
    "    p_column_name_wells: str\n",
    "    k_column_name_wells: str\n",
    "    observation_point_min_elevation_for_training: int\n",
    "    observation_point_max_elevation_for_validation: int\n",
    "    number_of_collocation_points: int\n",
    "    qN: float\n",
    "    Lambda: float\n",
    "    fix_random_numbers: bool\n",
    "    seed: int\n",
    "    first_optimization_mode: OptimizationMode\n",
    "    second_optimization_mode: OptimizationMode\n",
    "    third_optimization_mode: OptimizationMode\n",
    "    fourth_optimization_mode: OptimizationMode\n",
    "    first_optimization_lr: tf.keras.optimizers.schedules.LearningRateSchedule\n",
    "    second_optimization_lr: tf.keras.optimizers.schedules.LearningRateSchedule\n",
    "    third_optimization_lr: tf.keras.optimizers.schedules.LearningRateSchedule\n",
    "    fourth_optimization_lr: tf.keras.optimizers.schedules.LearningRateSchedule\n",
    "    first_optimization_steps: int = 1\n",
    "    second_optimization_steps: int = 1\n",
    "    third_optimization_steps: int = 1\n",
    "    fourth_optimization_steps: int = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PINN's training data and settings\n",
    "task_config = TaskConfiguration(\n",
    "    dtype=\"float32\",\n",
    "    data_csv=\"Reference_model.csv\",\n",
    "    x1_column_name=\"X_Easting\",\n",
    "    x2_column_name=\"Y_Northing\",\n",
    "    x3_column_name=\"Elevation\",\n",
    "    T_column_name=\"T_degC\",\n",
    "    p_column_name=\"P_Pa\",\n",
    "    k_column_name=\"log10PER\",\n",
    "    N_x1=18,\n",
    "    N_x2=11,\n",
    "    N_x3=18,\n",
    "    data_wells_csv=\"Welldata_30wells.csv\",\n",
    "    x1_column_name_wells=\"X_Easting\",\n",
    "    x2_column_name_wells=\"Y_Northing\",\n",
    "    x3_column_name_wells=\"Elevation\",\n",
    "    T_column_name_wells=\"T_degC\",\n",
    "    p_column_name_wells=\"P_Pa\",\n",
    "    k_column_name_wells=\"log10PER\",\n",
    "    observation_point_min_elevation_for_training=-1600,\n",
    "    observation_point_max_elevation_for_validation=-1600,\n",
    "    number_of_collocation_points=2000,\n",
    "    qN=0.6,\n",
    "    Lambda=2.0,\n",
    "    fix_random_numbers=True,\n",
    "    seed=211*17,\n",
    "    first_optimization_mode=OptimizationMode.TF,\n",
    "    first_optimization_steps=7000,\n",
    "    first_optimization_lr=tf.keras.optimizers.schedules.PiecewiseConstantDecay([3000,10000],[5e-3,2.5e-3,5e-4]),\n",
    "    second_optimization_mode=OptimizationMode.TF,\n",
    "    second_optimization_steps=2000,\n",
    "    second_optimization_lr=tf.keras.optimizers.schedules.PiecewiseConstantDecay([3000,10000],[5e-4,1e-4,5e-5]),\n",
    "    third_optimization_mode=OptimizationMode.TF,\n",
    "    third_optimization_steps=6000,\n",
    "    third_optimization_lr=tf.keras.optimizers.schedules.PiecewiseConstantDecay([3000,10000],[5e-4,1e-4,5e-5]),\n",
    "    fourth_optimization_mode=OptimizationMode.LBFGS,\n",
    "    fourth_optimization_steps=500,\n",
    "    fourth_optimization_lr=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Sets the directory name with a timestamp\n",
    "# The format of the timestamp is \"yyMMdd-hhmmss\".\n",
    "timestamp = datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "\n",
    "# The name of output directory is set to be \"result-{timestamp}\" .\n",
    "output_dir = Path(f\"result-{timestamp}\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict\n",
    "import json\n",
    "\n",
    "# The task settings are saved in JSON format directly under the output directory.\n",
    "task_config_save_path = output_dir/'task_config.json'\n",
    "with open(task_config_save_path, 'w', encoding='utf-8') as jsonfile:\n",
    "    json.dump(asdict(task_config), jsonfile, indent=2, default=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix random number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if task_config.fix_random_numbers:\n",
    "    random.seed(task_config.seed)\n",
    "    np.random.seed(task_config.seed)\n",
    "    tf.random.set_seed(task_config.seed)\n",
    "    tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_data(\n",
    "    csv_path: str,\n",
    "    x1_column_name=\"X_Easting\",\n",
    "    x2_column_name=\"Y_Northing\",\n",
    "    x3_column_name=\"Elevation\",\n",
    "    T_column_name=\"T_degC\",\n",
    "    p_column_name=\"P_Pa\",\n",
    "    k_column_name=\"log10PER\",\n",
    "    dtype=\"float32\",\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Read CSV file and extract coordinates and quantities\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_path: str\n",
    "        Path of the CSV file\n",
    "    x1_column_name: str\n",
    "        Column name for x axis in the CSV file\n",
    "    x2_column_name: str\n",
    "        Column name for y axis in the CSV file\n",
    "    x3_column_name: str\n",
    "        Column name for z axis (elevation) in the CSV file\n",
    "    T_column_name: str\n",
    "        Column name for temperature in the CSV file\n",
    "    p_column_name: str\n",
    "        Column name for pressure in the CSV file\n",
    "    k_column_name: str\n",
    "        Column name for permeability in the CSV file\n",
    "    dtype: str\n",
    "        Data type\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (X, T_star, p_star, k_star)\n",
    "        X: XYZ coordinates, shape = (num_points, 3)\n",
    "        T: Temperature, shape = (num_points, 1)\n",
    "        p: Pressure, shape = (num_points, 1)\n",
    "        k: Logarithm of permeability, shape = (num_points, 1)\n",
    "    \"\"\"\n",
    "    csv_data = pd.read_csv(csv_path)\n",
    "\n",
    "    X = np.array((csv_data[x1_column_name], csv_data[x2_column_name], csv_data[x3_column_name]), dtype=dtype).T\n",
    "\n",
    "    T = np.array(csv_data[T_column_name], dtype=dtype).reshape(X.shape[0], 1)\n",
    "    p = np.array(csv_data[p_column_name], dtype=dtype).reshape(X.shape[0], 1)\n",
    "    k = np.array(csv_data[k_column_name], dtype=dtype).reshape(X.shape[0], 1)\n",
    "\n",
    "    return X, T, p, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read reference data from the specified CSV file\n",
    "X, T_true, p_true, k_true = read_csv_data(\n",
    "    task_config.data_csv,\n",
    "    task_config.x1_column_name,\n",
    "    task_config.x2_column_name,\n",
    "    task_config.x3_column_name,\n",
    "    task_config.T_column_name,\n",
    "    task_config.p_column_name,\n",
    "    task_config.k_column_name)\n",
    "\n",
    "# Read well data from the specified CSV file\n",
    "X_star, T_star, p_star, k_star = read_csv_data(\n",
    "    task_config.data_wells_csv,\n",
    "    task_config.x1_column_name_wells,\n",
    "    task_config.x2_column_name_wells,\n",
    "    task_config.x3_column_name_wells,\n",
    "    task_config.T_column_name_wells,\n",
    "    task_config.p_column_name_wells,\n",
    "    task_config.k_column_name_wells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define normalize functions for temperature, pressure, and permeability\n",
    "from modules.normalizer import MinMaxNormalizer\n",
    "\n",
    "T_normalizer = MinMaxNormalizer(min_value=T_star.min(), max_value=T_star.max())\n",
    "p_normalizer = MinMaxNormalizer(min_value=p_star.min(), max_value=p_star.max())\n",
    "k_normalizer = MinMaxNormalizer(min_value=k_star.min(), max_value=k_star.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_true_data(\n",
    "    X: np.ndarray, T_true: np.ndarray, p_true: np.ndarray, k_true: np.ndarray,\n",
    "    savefig_prefix: Optional[str] = None,\n",
    "):\n",
    "\n",
    "    for true_values, var_name, cmap_name, vmin, vmax in [\n",
    "        (T_true, \"T_true\", \"hot\", 15, 350),\n",
    "        # (T_true, \"T_true\", \"RdYlBu\"),\n",
    "        (p_true, \"p_true\", \"cool\", 1e5, 2.5e7),\n",
    "        (k_true, \"k_true\", \"jet\", -17, -12),\n",
    "    ]:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "        pl = ax.scatter(xs=X[:, 0], ys=X[:, 1], zs=X[:, 2], vmin=vmin, vmax=vmax, c=true_values, s=30, marker=\"s\", cmap=cmap_name)\n",
    "        plt.colorbar(pl)\n",
    "        ax.set_title(var_name)\n",
    "        ax.set_xlabel(\"x\")\n",
    "        ax.set_ylabel(\"y\")\n",
    "        plt.tight_layout()\n",
    "        if savefig_prefix:\n",
    "            savefig_path = Path(f\"{savefig_prefix}{var_name}.png\")\n",
    "            savefig_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            # Save figure\n",
    "            plt.savefig(savefig_path)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Plot reference data\n",
    "plot_true_data(\n",
    "    X, T_true, p_true, k_true,\n",
    "    savefig_prefix=f'{output_dir.as_posix()}/figures/',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.neuralnet import PINN_NeuralNet3\n",
    "\n",
    "# Lower bounds\n",
    "lb = X.min(0)\n",
    "# Upper bounds\n",
    "ub = X.max(0)\n",
    "\n",
    "# Initialize PINN model\n",
    "model = PINN_NeuralNet3(lb, ub)\n",
    "model.build(input_shape=(None, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary of indices that indicate which of the elements of the tuples output by NN correspond to each variable\n",
    "NN_output_indices = {\"T\": 0, \"p\": 1, \"k\": 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract well data below the specified threshold as training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract well data points equal to and deeper than the specified elevation threshold \n",
    "x3_match_train = (\n",
    "    X_star[:, 2] >= task_config.observation_point_min_elevation_for_training,\n",
    ")\n",
    "\n",
    "train_obs_point_indices = np.flatnonzero(x3_match_train)\n",
    "\n",
    "# Coordinates, temperatures, pressurs, and permeabilities at the extracted indices\n",
    "train_X_obs = X_star[train_obs_point_indices]\n",
    "train_T_obs = T_star[train_obs_point_indices]\n",
    "train_p_obs = p_star[train_obs_point_indices]\n",
    "train_k_obs = k_star[train_obs_point_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot locations for observation points\n",
    "def plot_points(points: np.ndarray, x1_range=(None, None), x2_range=(None, None), x3_range=(None, None)):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.scatter(points[:, 0], points[:, 1], points[:, 2], s=10)\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.set_xlim(x1_range)\n",
    "    ax.set_ylim(x2_range)\n",
    "    ax.set_zlim(x3_range)\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_points(\n",
    "    train_X_obs,\n",
    "    x1_range=(X[:, 0].min(), X[:, 0].max()),\n",
    "    x2_range=(X[:, 1].min(), X[:, 1].max()),\n",
    "    x3_range=(X[:, 2].min(), X[:, 2].max()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.loss import ObservationLoss\n",
    "\n",
    "# Define loss component for each observed quantities (temperature, pressure, permeability)\n",
    "T_obs_loss = ObservationLoss(\n",
    "    observed_point_coordinates=train_X_obs,\n",
    "    observed_values=T_normalizer.normalize(train_T_obs),\n",
    "    NN_output_index=NN_output_indices[\"T\"],\n",
    ")\n",
    "p_obs_loss = ObservationLoss(\n",
    "    observed_point_coordinates=train_X_obs,\n",
    "    observed_values=p_normalizer.normalize(train_p_obs),\n",
    "    NN_output_index=NN_output_indices[\"p\"],\n",
    ")\n",
    "k_obs_loss = ObservationLoss(\n",
    "    observed_point_coordinates=train_X_obs,\n",
    "    observed_values=k_normalizer.normalize(train_k_obs),\n",
    "    NN_output_index=NN_output_indices[\"k\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract well data below the specified threshold as validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract well data points deeper than the specified elevation threshold \n",
    "x3_match_val = (\n",
    "    X_star[:, 2] <= task_config.observation_point_max_elevation_for_validation,\n",
    ")\n",
    "\n",
    "val_obs_point_indices = np.flatnonzero(x3_match_val)\n",
    "\n",
    "# Coordinates, temperatures, pressurs, and permeabilities at the extracted indices\n",
    "val_X_obs = X_star[val_obs_point_indices]\n",
    "val_T_obs = T_star[val_obs_point_indices]\n",
    "val_p_obs = p_star[val_obs_point_indices]\n",
    "val_k_obs = k_star[val_obs_point_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot locations of validation data points\n",
    "plot_points(\n",
    "    val_X_obs,\n",
    "    x1_range=(X[:, 0].min(), X[:, 0].max()),\n",
    "    x2_range=(X[:, 1].min(), X[:, 1].max()),\n",
    "    x3_range=(X[:, 2].min(), X[:, 2].max()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the observed values used in training, set the loss of the prediction error for each variable.\n",
    "val_T_obs_loss = ObservationLoss(\n",
    "    observed_point_coordinates=val_X_obs,\n",
    "    observed_values=T_normalizer.normalize(val_T_obs),\n",
    "    NN_output_index=NN_output_indices[\"T\"],\n",
    ")\n",
    "val_p_obs_loss = ObservationLoss(\n",
    "    observed_point_coordinates=val_X_obs,\n",
    "    observed_values=p_normalizer.normalize(val_p_obs),\n",
    "    NN_output_index=NN_output_indices[\"p\"],\n",
    ")\n",
    "val_k_obs_loss = ObservationLoss(\n",
    "    observed_point_coordinates=val_X_obs,\n",
    "    observed_values=k_normalizer.normalize(val_k_obs),\n",
    "    NN_output_index=NN_output_indices[\"k\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss for boundary conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upper surface boundary condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.loss import DirichletBoundaryConditionLoss\n",
    "\n",
    "# Extract point indices at the upper surface boundary\n",
    "x3 = X[:, 2]\n",
    "top_boundary_point_indices = np.flatnonzero(x3 == x3.max())\n",
    "X_top_boundary = X[top_boundary_point_indices]\n",
    "\n",
    "# Dirichlet boundary condition for temperature\n",
    "TbD_loss = DirichletBoundaryConditionLoss(\n",
    "    boundary_coordinates=X_top_boundary,\n",
    "    boundary_values=T_normalizer.normalize(T_true[top_boundary_point_indices]),\n",
    "    NN_output_index=NN_output_indices[\"T\"],\n",
    ")\n",
    "\n",
    "# Dirichlet boundary condition for pressure\n",
    "pbD_loss = DirichletBoundaryConditionLoss(\n",
    "    boundary_coordinates=X_top_boundary,\n",
    "    boundary_values=p_normalizer.normalize(p_true[top_boundary_point_indices]),\n",
    "    NN_output_index=NN_output_indices[\"p\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Side boundary condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.loss import NeumannBoundaryConditionLoss\n",
    "\n",
    "# Extract point indices at the east and west boundaries\n",
    "x1 = X[:, 0]\n",
    "west_boundary_point_indices = np.flatnonzero(x1 == x1.min())\n",
    "east_boundary_point_indices = np.flatnonzero(x1 == x1.max())\n",
    "\n",
    "# Normal vector at the west boundary\n",
    "west_boundary_normal_vectors = np.tile(\n",
    "    np.array([-1.0, 0.0, 0.0]), (len(west_boundary_point_indices), 1)\n",
    ")\n",
    "\n",
    "# Normal vector at the east boundary\n",
    "east_boundary_normal_vectors = np.tile(\n",
    "    np.array([1.0, 0.0, 0.0]), (len(east_boundary_point_indices), 1)\n",
    ")\n",
    "\n",
    "# Extract point indices at the north and south boundaries\n",
    "x2 = X[:, 1]\n",
    "south_boundary_point_indices = np.flatnonzero(x2 == x2.min())\n",
    "north_boundary_point_indices = np.flatnonzero(x2 == x2.max())\n",
    "\n",
    "# Normal vector at the south boundary\n",
    "south_boundary_normal_vectors = np.tile(\n",
    "    np.array([0.0, -1.0, 0.0]), (len(south_boundary_point_indices), 1)\n",
    ")\n",
    "\n",
    "# Normal vector at the north boundary\n",
    "north_boundary_normal_vectors = np.tile(\n",
    "    np.array([0.0, 1.0, 0.0]), (len(north_boundary_point_indices), 1)\n",
    ")\n",
    "\n",
    "# Neumann boundary condition for pressure (mass flow) \n",
    "pbN_loss = NeumannBoundaryConditionLoss(\n",
    "    boundary_coordinates=np.concatenate(\n",
    "        [X[west_boundary_point_indices], X[east_boundary_point_indices],\n",
    "         X[south_boundary_point_indices], X[north_boundary_point_indices]]\n",
    "    ),\n",
    "    boundary_values=np.zeros(\n",
    "        shape=(len(west_boundary_point_indices)\n",
    "               + len(east_boundary_point_indices)\n",
    "               + len(south_boundary_point_indices)\n",
    "               + len(north_boundary_point_indices), 1)\n",
    "    ),\n",
    "    normal_vector=np.concatenate(\n",
    "        [west_boundary_normal_vectors, east_boundary_normal_vectors,\n",
    "         south_boundary_normal_vectors, north_boundary_normal_vectors]\n",
    "    ),\n",
    "    normalizer=p_normalizer,\n",
    "    NN_output_index=NN_output_indices[\"p\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bottom boundary condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract point indices at the bottom boundary\n",
    "x3 = X[:, 2]\n",
    "bottom_boundary_point_indices = np.flatnonzero(x3 == x3.min())\n",
    "X_bottom_boundary = X[bottom_boundary_point_indices]\n",
    "\n",
    "# Noraml vector at the bottom boundary condition\n",
    "bottom_boundary_normal_vectors = np.tile(\n",
    "    np.array([0.0, 0.0, -1.0]), (len(bottom_boundary_point_indices), 1)\n",
    ")\n",
    "\n",
    "# Temperature gradient at the bottom\n",
    "dTdn = task_config.qN / task_config.Lambda\n",
    "\n",
    "# Neumann boundary condition for temperature (heat flow) at the bottom\n",
    "TbN_loss = NeumannBoundaryConditionLoss(\n",
    "    boundary_coordinates=X_bottom_boundary,\n",
    "    boundary_values=dTdn * np.ones(shape=(len(bottom_boundary_point_indices), 1)),\n",
    "    normal_vector=bottom_boundary_normal_vectors,\n",
    "    normalizer=T_normalizer,\n",
    "    NN_output_index=NN_output_indices[\"T\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss for physical laws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collocation points のみ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.loss import (\n",
    "    PhysicsInformedLoss_r1,\n",
    "    PhysicsInformedLoss_r2,\n",
    ")\n",
    "\n",
    "# Select collocation points where physical laws are calculated\n",
    "\n",
    "collocation_point_indices = np.random.choice(\n",
    "    np.arange(len(X)), task_config.number_of_collocation_points, replace=False\n",
    ")\n",
    "\n",
    "PI1_loss = PhysicsInformedLoss_r1(\n",
    "    collocation_point_coordinates=X[collocation_point_indices],\n",
    "    T_normalizer=T_normalizer,\n",
    "    p_normalizer=p_normalizer,\n",
    "    k_normalizer=k_normalizer,\n",
    "    dtype=task_config.dtype,\n",
    ")\n",
    "\n",
    "PI2_loss = PhysicsInformedLoss_r2(\n",
    "    collocation_point_coordinates=X[collocation_point_indices],\n",
    "    T_normalizer=T_normalizer,\n",
    "    p_normalizer=p_normalizer,\n",
    "    k_normalizer=k_normalizer,\n",
    "    Lambda=task_config.Lambda,\n",
    "    dtype=task_config.dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot locations of collocation points\n",
    "plot_points(\n",
    "    X[collocation_point_indices],\n",
    "    x1_range=(X[:, 0].min(), X[:, 0].max()),\n",
    "    x2_range=(X[:, 1].min(), X[:, 1].max()),\n",
    "    x3_range=(X[:, 2].min(), X[:, 2].max()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Whole grid (for test evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the grid to calculate physical laws in all target domain\n",
    "# These are used for test evaluation, not for training\n",
    "\n",
    "all_PI1_loss = PhysicsInformedLoss_r1(\n",
    "    collocation_point_coordinates=X,\n",
    "    T_normalizer=T_normalizer,\n",
    "    p_normalizer=p_normalizer,\n",
    "    k_normalizer=k_normalizer,\n",
    ")\n",
    "\n",
    "all_PI2_loss = PhysicsInformedLoss_r2(\n",
    "    collocation_point_coordinates=X,\n",
    "    T_normalizer=T_normalizer,\n",
    "    p_normalizer=p_normalizer,\n",
    "    k_normalizer=k_normalizer,\n",
    "    Lambda=task_config.Lambda,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.solver_callback import (\n",
    "    PrintCurrentLossAndMetricsCallback,\n",
    "    CsvLoggerCallback,\n",
    "    NNWeightsCheckpointCallback,\n",
    "    AllGridPredictionCallback,\n",
    ")\n",
    "\n",
    "# Define loss function\n",
    "loss_functions_stage1 = {\n",
    "    \"train_T_MSE\": T_obs_loss,\n",
    "    \"train_p_MSE\": p_obs_loss,\n",
    "    \"train_k_MSE\": k_obs_loss,\n",
    "}\n",
    "\n",
    "# Define loss components to be monitored\n",
    "# These loss components are not used for training\n",
    "metrics_stage1 = {\n",
    "    \"val_T_MSE\": val_T_obs_loss,\n",
    "    \"val_p_MSE\": val_p_obs_loss,\n",
    "    \"val_k_MSE\": val_k_obs_loss,\n",
    "    \"TbD_loss\": TbD_loss,\n",
    "    \"pbD_loss\": pbD_loss,\n",
    "    \"TbN_loss\": TbN_loss,\n",
    "    \"pbN_loss\": pbN_loss,\n",
    "    \"PI1_loss\": PI1_loss,\n",
    "    \"PI2_loss\": PI2_loss,\n",
    "    \"all_PI1_loss\": all_PI1_loss,\n",
    "    \"all_PI2_loss\": all_PI2_loss,\n",
    "}\n",
    "\n",
    "# Callback setting\n",
    "history_csv_path_stage1 = output_dir / \"history_stage1.csv\"\n",
    "predicts_save_dir_stage1 = output_dir / \"save_predicts_stage1\"\n",
    "callbacks_stage1 = [\n",
    "    PrintCurrentLossAndMetricsCallback(interval=100),\n",
    "    CsvLoggerCallback(csv_path=history_csv_path_stage1),\n",
    "    NNWeightsCheckpointCallback(\n",
    "        checkpoint_dir=output_dir / \"checkpoints_stage1\", interval=1000\n",
    "    ),\n",
    "    AllGridPredictionCallback(\n",
    "        save_prediction_dir=predicts_save_dir_stage1,\n",
    "        interval=1000,\n",
    "        all_grid_point_coordinates=X,\n",
    "        T_normalizer=T_normalizer,\n",
    "        p_normalizer=p_normalizer,\n",
    "        k_normalizer=k_normalizer,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the first training\n",
    "from modules.solver import PINNSolver, PINNSolverLBfgs\n",
    "\n",
    "\n",
    "if task_config.first_optimization_mode == OptimizationMode.TF:\n",
    "    print('Start training with a TF optimizer')\n",
    "    optimizer_stage1 = tf.keras.optimizers.Adam(\n",
    "        learning_rate=task_config.first_optimization_lr\n",
    "    )\n",
    "    solver_stage1 = PINNSolver(\n",
    "        model=model,\n",
    "        optimizer=optimizer_stage1,\n",
    "        loss_functions=loss_functions_stage1,\n",
    "        metrics=metrics_stage1,\n",
    "    )\n",
    "    solver_stage1.solve(\n",
    "        n_steps=task_config.first_optimization_steps,\n",
    "        callbacks=callbacks_stage1,\n",
    "    )\n",
    "\n",
    "elif task_config.first_optimization_mode == OptimizationMode.LBFGS:\n",
    "    print('Start training with an L-BFGS optimizer')\n",
    "    solver_stage1 = PINNSolverLBfgs(\n",
    "        model=model,\n",
    "        loss_functions=loss_functions_stage1,\n",
    "        metrics=metrics_stage1,\n",
    "        lbfgs_kwargs=dict()\n",
    "    )\n",
    "    solver_stage1.solve(\n",
    "        n_steps=task_config.first_optimization_steps,\n",
    "        callbacks=callbacks_stage1,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    raise ValueError(f'first_optimization_mode must be \"TF\" or \"LBFGS\", but {task_config.first_optimization_mode}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(\n",
    "    history_csv_path: str,\n",
    "    target_metrics: List[str] = [\"train_loss\"],\n",
    "    savefig_path: Optional[str] = None,\n",
    "):\n",
    "    # Read loss history\n",
    "    history_df = pd.read_csv(history_csv_path, index_col=0)\n",
    "\n",
    "    # Plot learning curve\n",
    "    plt.figure()\n",
    "    history_df.plot(\n",
    "        y=target_metrics,\n",
    "        use_index=True,\n",
    "        title=\"Learning Curve\",\n",
    "        figsize=(10, 4),\n",
    "        grid=True,\n",
    "        xlim=(0, None),\n",
    "        ylim=(0, None),\n",
    "        logy=False,  # If vertical axis is plotted in logarithm scale, set it \"True\"\n",
    "    )\n",
    "    if savefig_path:\n",
    "        Path(savefig_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        # Save figure\n",
    "        plt.savefig(savefig_path)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot learning curve\n",
    "plot_learning_curve(\n",
    "    history_csv_path_stage1,\n",
    "    savefig_path=(output_dir / \"figures_stage1/learning_curve.png\").as_posix(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(prediction_csv_path: str, savefig_prefix: Optional[str] = None, dtype='float32'):\n",
    "    # Predicted values are read from the csv file\n",
    "    prediction_df = pd.read_csv(prediction_csv_path, index_col=None)\n",
    "\n",
    "    X = np.array((prediction_df[\"x1\"], prediction_df[\"x2\"], prediction_df[\"x3\"]), dtype=dtype).T\n",
    "\n",
    "    for var_name, cmap_name, vmin, vmax in [\n",
    "        (\"T_pred\", \"hot\", 15, 350),\n",
    "        (\"p_pred\", \"cool\", 1e5, 2.5e7),\n",
    "        (\"k_pred\", \"jet\", -17, -12),\n",
    "    ]:\n",
    "        var = np.array(prediction_df[var_name], dtype=dtype).reshape(X.shape[0], 1)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "        pl = ax.scatter(\n",
    "            xs=X[:, 0], ys=X[:, 1], zs=X[:, 2], vmin=vmin, vmax=vmax, c=var, \n",
    "            s=30, marker=\"s\", cmap=cmap_name)\n",
    "        plt.colorbar(pl)\n",
    "        ax.set_title(var_name)\n",
    "        ax.set_xlabel(\"x\")\n",
    "        ax.set_ylabel(\"y\")\n",
    "        plt.tight_layout()\n",
    "        if savefig_prefix:\n",
    "            savefig_path = Path(f\"{savefig_prefix}{var_name}.png\")\n",
    "            savefig_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            # Save figure\n",
    "            plt.savefig(savefig_path)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Plot the spatial distribution of the predicted quantities (T, P, and log10K)\n",
    "plot_prediction(\n",
    "    f\"{predicts_save_dir_stage1}/predicted_{solver_stage1.step_counter}.csv\",\n",
    "    savefig_prefix=f\"{output_dir.as_posix()}/figures_stage1/\",\n",
    "    dtype=task_config.dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "loss_functions_stage2 = {\n",
    "    \"train_T_MSE\": T_obs_loss,\n",
    "    \"train_p_MSE\": p_obs_loss,\n",
    "    \"train_k_MSE\": k_obs_loss,\n",
    "    \"PI1_loss\": PI1_loss,\n",
    "    \"PI2_loss\": PI2_loss,\n",
    "}\n",
    "\n",
    "# Weights of loss component from the loss history of previous training\n",
    "last_losses = solver_stage1.get_last_loss_and_metrics()\n",
    "average_Tpk_loss = np.average(\n",
    "    [\n",
    "        last_losses[\"train_T_MSE\"],\n",
    "        last_losses[\"train_p_MSE\"],\n",
    "        last_losses[\"train_k_MSE\"],\n",
    "    ]\n",
    ")\n",
    "loss_weights_stage2 = {\n",
    "    \"PI1_loss\": 0,\n",
    "    \"PI2_loss\": 0,\n",
    "}\n",
    "\n",
    "# Set loss component to be monitored during the training\n",
    "# These loss components do not influence NN training\n",
    "metrics_stage2 = {\n",
    "    \"val_T_MSE\": val_T_obs_loss,\n",
    "    \"val_p_MSE\": val_p_obs_loss,\n",
    "    \"val_k_MSE\": val_k_obs_loss,\n",
    "    \"TbD_loss\": TbD_loss,\n",
    "    \"pbD_loss\": pbD_loss,\n",
    "    \"TbN_loss\": TbN_loss,\n",
    "    \"pbN_loss\": pbN_loss,\n",
    "    \"all_PI1_loss\": all_PI1_loss,\n",
    "    \"all_PI2_loss\": all_PI2_loss,\n",
    "}\n",
    "\n",
    "# Callback setting\n",
    "history_csv_path_stage2 = output_dir / \"history_stage2.csv\"\n",
    "predicts_save_dir_stage2 = output_dir / \"save_predicts_stage2\"\n",
    "callbacks_stage2 = [\n",
    "    PrintCurrentLossAndMetricsCallback(interval=100),\n",
    "    CsvLoggerCallback(csv_path=history_csv_path_stage2),\n",
    "    NNWeightsCheckpointCallback(\n",
    "        checkpoint_dir=output_dir / \"checkpoints_stage2\", interval=100\n",
    "    ),\n",
    "    AllGridPredictionCallback(\n",
    "        save_prediction_dir=predicts_save_dir_stage2,\n",
    "        interval=100,\n",
    "        all_grid_point_coordinates=X,\n",
    "        T_normalizer=T_normalizer,\n",
    "        p_normalizer=p_normalizer,\n",
    "        k_normalizer=k_normalizer,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the second training\n",
    "if task_config.second_optimization_mode == OptimizationMode.TF:\n",
    "    optimizer_stage2 = tf.keras.optimizers.Adam(\n",
    "        learning_rate=task_config.second_optimization_lr\n",
    "    )\n",
    "    solver_stage2 = PINNSolver(\n",
    "        model=model,\n",
    "        optimizer=optimizer_stage2,\n",
    "        loss_functions=loss_functions_stage2,\n",
    "        loss_weights=loss_weights_stage2,\n",
    "        metrics=metrics_stage2,\n",
    "    )\n",
    "    solver_stage2.solve(\n",
    "        n_steps=task_config.second_optimization_steps,\n",
    "        callbacks=callbacks_stage2,\n",
    "    )\n",
    "\n",
    "elif task_config.second_optimization_mode == OptimizationMode.LBFGS:\n",
    "    print('Start training with an L-BFGS optimizer')\n",
    "    solver_stage2 = PINNSolverLBfgs(\n",
    "        model=model,\n",
    "        loss_functions=loss_functions_stage2,\n",
    "        metrics=metrics_stage2,\n",
    "        loss_weights=loss_weights_stage2,\n",
    "        lbfgs_kwargs=dict()\n",
    "    )\n",
    "    solver_stage2.solve(\n",
    "        n_steps=task_config.second_optimization_steps,\n",
    "        callbacks=callbacks_stage2,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    raise ValueError(f'second_optimization_mode must be \"TF\" or \"LBFGS\", but {task_config.second_optimization_mode}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "plot_learning_curve(\n",
    "    history_csv_path_stage2,\n",
    "    savefig_path=(output_dir / \"figures_stage2/learning_curve.png\").as_posix(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the spatial distribution of the predicted quantities\n",
    "plot_prediction(\n",
    "    f\"{predicts_save_dir_stage2}/predicted_{solver_stage2.step_counter}.csv\",\n",
    "    savefig_prefix=f\"{output_dir.as_posix()}/figures_stage2/\",\n",
    "    dtype=task_config.dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "loss_functions_stage3 = {\n",
    "    \"train_T_MSE\": T_obs_loss,\n",
    "    \"train_p_MSE\": p_obs_loss,\n",
    "    \"train_k_MSE\": k_obs_loss,\n",
    "    \"TbD_loss\": TbD_loss,\n",
    "    \"pbD_loss\": pbD_loss,\n",
    "    \"PI1_loss\": PI1_loss,\n",
    "    \"PI2_loss\": PI2_loss,\n",
    "}\n",
    "\n",
    "# Weights of loss component from the loss history of previous training\n",
    "last_losses = solver_stage2.get_last_loss_and_metrics()\n",
    "average_Tpk_loss = np.average(\n",
    "    [\n",
    "        last_losses[\"train_T_MSE\"],\n",
    "        last_losses[\"train_p_MSE\"],\n",
    "        last_losses[\"train_k_MSE\"],\n",
    "    ]\n",
    ")\n",
    "loss_weights_stage3 = {\n",
    "    \"PI1_loss\": float(average_Tpk_loss / last_losses[\"PI1_loss\"]),\n",
    "    \"PI2_loss\": float(average_Tpk_loss / last_losses[\"PI2_loss\"]),\n",
    "}\n",
    "\n",
    "# Set loss component to be monitored during the training\n",
    "# These loss components do not influence NN training.\n",
    "metrics_stage3 = {\n",
    "    \"val_T_MSE\": val_T_obs_loss,\n",
    "    \"val_p_MSE\": val_p_obs_loss,\n",
    "    \"val_k_MSE\": val_k_obs_loss,\n",
    "    \"all_PI1_loss\": all_PI1_loss,\n",
    "    \"all_PI2_loss\": all_PI2_loss,\n",
    "}\n",
    "\n",
    "# Callback setting\n",
    "history_csv_path_stage3 = output_dir / \"history_stage3.csv\"\n",
    "predicts_save_dir_stage3 = output_dir / \"save_predicts_stage3\"\n",
    "callbacks_stage3 = [\n",
    "    PrintCurrentLossAndMetricsCallback(interval=100),\n",
    "    CsvLoggerCallback(csv_path=history_csv_path_stage3),\n",
    "    NNWeightsCheckpointCallback(\n",
    "        checkpoint_dir=output_dir / \"checkpoints_stage3\", interval=1000\n",
    "    ),\n",
    "    AllGridPredictionCallback(\n",
    "        save_prediction_dir=predicts_save_dir_stage3,\n",
    "        interval=1000,\n",
    "        all_grid_point_coordinates=X,\n",
    "        T_normalizer=T_normalizer,\n",
    "        p_normalizer=p_normalizer,\n",
    "        k_normalizer=k_normalizer,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the third training\n",
    "if task_config.third_optimization_mode == OptimizationMode.TF:\n",
    "    optimizer_stage3 = tf.keras.optimizers.Adam(\n",
    "        learning_rate=task_config.third_optimization_lr\n",
    "    )\n",
    "    solver_stage3 = PINNSolver(\n",
    "        model=model,\n",
    "        optimizer=optimizer_stage3,\n",
    "        loss_functions=loss_functions_stage3,\n",
    "        loss_weights=loss_weights_stage3,\n",
    "        metrics=metrics_stage3,\n",
    "    )\n",
    "    solver_stage3.solve(\n",
    "        n_steps=task_config.third_optimization_steps,\n",
    "        callbacks=callbacks_stage3,\n",
    "    )\n",
    "\n",
    "elif task_config.third_optimization_mode == OptimizationMode.LBFGS:\n",
    "    print('Start training with an L-BFGS optimizer')\n",
    "    solver_stage3 = PINNSolverLBfgs(\n",
    "        model=model,\n",
    "        loss_functions=loss_functions_stage3,\n",
    "        loss_weights=loss_weights_stage3,\n",
    "        metrics=metrics_stage3,\n",
    "        lbfgs_kwargs=dict()\n",
    "    )\n",
    "    solver_stage3.solve(\n",
    "        n_steps=task_config.third_optimization_steps,\n",
    "        callbacks=callbacks_stage3,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    raise ValueError(f'third_optimization_mode must be \"TF\" or \"LBFGS\", but {task_config.third_optimization_mode}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "plot_learning_curve(\n",
    "    history_csv_path_stage3,\n",
    "    savefig_path=(output_dir / \"figures_stage3/learning_curve.png\").as_posix(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the spatial distribution of the predicted quantities\n",
    "plot_prediction(\n",
    "    f\"{predicts_save_dir_stage3}/predicted_{solver_stage3.step_counter}.csv\",\n",
    "    savefig_prefix=f\"{output_dir.as_posix()}/figures_stage3/\",\n",
    "    dtype=task_config.dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "loss_functions_stage4 = {\n",
    "    \"train_T_MSE\": T_obs_loss,\n",
    "    \"train_p_MSE\": p_obs_loss,\n",
    "    \"train_k_MSE\": k_obs_loss,\n",
    "    \"TbD_loss\": TbD_loss,\n",
    "    \"pbD_loss\": pbD_loss,\n",
    "    \"PI1_loss\": PI1_loss,\n",
    "    \"PI2_loss\": PI2_loss,\n",
    "}\n",
    "\n",
    "# # Weights of loss component from the loss history of previous training\n",
    "last_losses = solver_stage3.get_last_loss_and_metrics()\n",
    "average_Tpk_loss = np.average(\n",
    "    [\n",
    "        last_losses[\"train_T_MSE\"],\n",
    "        last_losses[\"train_p_MSE\"],\n",
    "        last_losses[\"train_k_MSE\"],\n",
    "    ]\n",
    ")\n",
    "loss_weights_stage4 = {\n",
    "    \"PI1_loss\": float(average_Tpk_loss / last_losses[\"PI1_loss\"]),\n",
    "    \"PI2_loss\": float(average_Tpk_loss / last_losses[\"PI2_loss\"]),\n",
    "}\n",
    "\n",
    "# Set loss component to be monitored during the training\n",
    "# These loss components do not influence NN training\n",
    "metrics_stage4 = {\n",
    "    \"val_T_MSE\": val_T_obs_loss,\n",
    "    \"val_p_MSE\": val_p_obs_loss,\n",
    "    \"val_k_MSE\": val_k_obs_loss,\n",
    "    \"all_PI1_loss\": all_PI1_loss,\n",
    "    \"all_PI2_loss\": all_PI2_loss,\n",
    "}\n",
    "\n",
    "# Callback setting\n",
    "history_csv_path_stage4 = output_dir / \"history_stage4.csv\"\n",
    "predicts_save_dir_stage4 = output_dir / \"save_predicts_stage4\"\n",
    "callbacks_stage4 = [\n",
    "    PrintCurrentLossAndMetricsCallback(interval=100),\n",
    "    CsvLoggerCallback(csv_path=history_csv_path_stage4),\n",
    "    NNWeightsCheckpointCallback(\n",
    "        checkpoint_dir=output_dir / \"checkpoints_stage4\", interval=1000\n",
    "    ),\n",
    "    AllGridPredictionCallback(\n",
    "        save_prediction_dir=predicts_save_dir_stage4,\n",
    "        interval=1000,\n",
    "        all_grid_point_coordinates=X,\n",
    "        T_normalizer=T_normalizer,\n",
    "        p_normalizer=p_normalizer,\n",
    "        k_normalizer=k_normalizer,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the forth training\n",
    "if task_config.fourth_optimization_mode == OptimizationMode.TF:\n",
    "    optimizer_stage4 = tf.keras.optimizers.Adam(\n",
    "        learning_rate=task_config.fourth_optimization_lr\n",
    "    )\n",
    "    solver_stage4 = PINNSolver(\n",
    "        model=model,\n",
    "        optimizer=optimizer_stage4,\n",
    "        loss_functions=loss_functions_stage4,\n",
    "        loss_weights=loss_weights_stage4,\n",
    "        metrics=metrics_stage4,\n",
    "    )\n",
    "    solver_stage4.solve(\n",
    "        n_steps=task_config.fourth_optimization_steps,\n",
    "        callbacks=callbacks_stage4,\n",
    "    )\n",
    "\n",
    "elif task_config.fourth_optimization_mode == OptimizationMode.LBFGS:\n",
    "    print('Start training with an L-BFGS optimizer')\n",
    "    solver_stage4 = PINNSolverLBfgs(\n",
    "        model=model,\n",
    "        loss_functions=loss_functions_stage4,\n",
    "        loss_weights=loss_weights_stage4,\n",
    "        metrics=metrics_stage4,\n",
    "        lbfgs_kwargs=dict()\n",
    "    )\n",
    "    solver_stage4.solve(\n",
    "        n_steps=task_config.fourth_optimization_steps,\n",
    "        callbacks=callbacks_stage4,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    raise ValueError(f'fourth_optimization_mode must be \"TF\" or \"LBFGS\", but {task_config.fourth_optimization_mode}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "plot_learning_curve(\n",
    "    history_csv_path_stage4,\n",
    "    savefig_path=(output_dir / \"figures_stage4/learning_curve.png\").as_posix(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the spatial distribution of the predicted quantities\n",
    "plot_prediction(\n",
    "    f\"{predicts_save_dir_stage4}/predicted_{solver_stage4.step_counter}.csv\",\n",
    "    savefig_prefix=f\"{output_dir.as_posix()}/figures_stage4/\",\n",
    "    dtype=task_config.dtype\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
